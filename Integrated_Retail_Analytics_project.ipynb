{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "GF8Ens_Soomf",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "OH-pJp9IphqM",
        "PIIx-8_IphqN",
        "BZR9WyysphqO",
        "YJ55k-q6phqO",
        "U2RJ9gkRphqQ",
        "x-EpHcCOp1ci",
        "n3dbpmDWp1ck",
        "Ag9LCva-p1cl",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "NCkPyFgS13nf",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "49K5P_iCpZyH",
        "kLW572S8pZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "Iwf50b-R2tYG",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "qBMux9mC6MCf",
        "PB48tQ5y39QT",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "VFOzZv6IFROw",
        "sggqrum0YqOD",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "_-qAgymDpx6N",
        "YmAR6lGiIQqc",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "7vadI60GZKST",
        "wCA_YRiOwgCl",
        "fimhvqj6w2hO",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryayayayaa/-Smart-Store-Operations-Optimizing-Retail-with-Analytics/blob/main/Integrated_Retail_Analytics_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name - Smart Store Operations: Optimizing Retail with Analytics**    \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project, \"Smart Store Operations,\" applies advanced machine learning and data analysis techniques to enhance retail store performance. The core objective is to move from reactive to proactive decision-making by leveraging data. We'll build predictive models for sales forecasting, identify unusual sales patterns through anomaly detection, and segment customers and stores to enable personalized strategies. The project also incorporates external economic factors to provide a more holistic view of sales drivers. Ultimately, this work delivers data-driven recommendations for improving inventory management, marketing, and overall store operations."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retailers face significant challenges in optimizing store performance, managing inventory, and developing effective marketing strategies due to complex and dynamic market conditions. Traditional methods often fail to account for the interplay of various internal and external factors that influence sales. This leads to issues such as:\n",
        "\n",
        "Inaccurate Demand Forecasting: Without a systematic approach, stores struggle to predict future demand, resulting in stockouts or overstocking, which directly impacts revenue and customer satisfaction.\n",
        "\n",
        "Inefficient Marketing: Generic marketing campaigns fail to resonate with diverse customer segments, leading to wasted resources and missed sales opportunities.\n",
        "\n",
        "Lack of Actionable Insights: A vast amount of sales data is collected but not effectively analyzed to identify key trends, anomalies, or underlying drivers of performance.\n",
        "\n",
        "The problem is to create a robust, integrated analytics framework that provides a deeper understanding of sales dynamics. This framework must accurately forecast demand, reveal consumer behavior through segmentation, and deliver actionable, data-driven strategies to improve profitability and operational efficiency in a competitive retail landscape."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import zscore\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "import warnings\n",
        "import shap\n",
        "\n",
        "# ML Model -1\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# ML Model - 2\n",
        "import xgboost as xgb\n",
        "\n",
        "# ML Model - 3\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Suppress the deprecation warnings from jupyter_client\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"Successfully loaded!\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# This mounts your Google Drive to the Colab environment.\n",
        "# will be prompted to authorize access.\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "\n",
        "sales_path = '/content/gdrive/MyDrive/Smart Store Operations: Optimizing Retail with Analytics/sales data-set.csv'\n",
        "features_path = '/content/gdrive/MyDrive/Smart Store Operations: Optimizing Retail with Analytics/Features data set.csv'\n",
        "stores_path = '/content/gdrive/MyDrive/Smart Store Operations: Optimizing Retail with Analytics/stores data-set.csv'\n",
        "\n",
        "# Load the datasets into pandas DataFrames\n",
        "sales_df = pd.read_csv(sales_path)\n",
        "features_df = pd.read_csv(features_path)\n",
        "stores_df = pd.read_csv(stores_path)\n",
        "\n",
        "print(\"Datasets loaded successfully!\\n\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "print(\"--- Sales Data Set ---\")\n",
        "print(sales_df.head())\n",
        "\n",
        "print(\"\\n--- Features Data Set ---\")\n",
        "print(features_df.head())\n",
        "\n",
        "print(\"\\n--- Stores Data Set ---\")\n",
        "print(stores_df.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "print(\"\\n--- Dataset Dimensions ---\")\n",
        "\n",
        "print(f\"Sales Data Set: {sales_df.shape[0]} rows, {sales_df.shape[1]} columns\")\n",
        "\n",
        "print(f\"Features Data Set: {features_df.shape[0]} rows, {features_df.shape[1]} columns\")\n",
        "\n",
        "print(f\"Stores Data Set: {stores_df.shape[0]} rows, {stores_df.shape[1]} columns\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"--- Dataset Info ---\")\n",
        "dfs = {'Sales': sales_df,\n",
        "       'Features': features_df,\n",
        "       'Stores': stores_df}\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    print(f\"\\n{name} Dataset Info:\")\n",
        "    df.info()\n",
        "    print()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"\\n--- Dataset Duplicate Value Count ---\")\n",
        "for name, df in dfs.items():\n",
        "    print(f\"{name} Dataset Duplicate Rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"\\n--- Missing Values/Null Values Count ---\")\n",
        "for name, df in dfs.items():\n",
        "    print(f\"\\n{name} Dataset Missing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "    print()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "print(\"\\n--- Visualizing Missing Values ---\")\n",
        "for name, df in dfs.items():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "    plt.title(f'Missing Values in {name} Dataset')\n",
        "    plt.show()\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **sales data-set.csv:** This is your primary dataset, containing the Weekly_Sales which is the target variable you need to predict. It also links sales to specific Store and Dept for analysis. The Date and IsHoliday columns are important for time-series analysis and identifying holiday effects.\n",
        "\n",
        "* **stores data-set.csv:** This is a static lookup table that provides metadata for each Store, including its Type (e.g., A, B, C) and Size. This data will be crucial for creating store-level segments and features.\n",
        "\n",
        "* **Features data set.csv:** This dataset contains a variety of external factors that can influence sales. The most notable columns are the MarkDown columns, which will likely contain a lot of missing values (NaNs) as markdowns are not a weekly occurrence. The Temperature, Fuel_Price, CPI, and Unemployment data will be essential for building a more comprehensive forecasting model."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"--- Dataset Columns ---\")\n",
        "for name, df in dfs.items():\n",
        "    print(f\"\\n{name} Columns:\")\n",
        "    print(df.columns.tolist())\n",
        "    print()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"\\n--- Dataset Describe ---\")\n",
        "for name, df in dfs.items():\n",
        "    print(f\"\\n{name} Describe:\")\n",
        "    print(df.describe())\n",
        "    print()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variables Description**\n",
        "---\n",
        "\n",
        "### `sales data-set.csv`\n",
        "\n",
        "* **Store**: The store number identifier (integer).\n",
        "* **Dept**: The department number identifier (integer).\n",
        "* **Date**: The week of sales (date format).\n",
        "* **Weekly_Sales**: The sales for the given department in the given store for the specified week. This is the **target variable** for your forecasting models.\n",
        "* **IsHoliday**: A boolean flag indicating whether the week is a special holiday week (`True` or `False`).\n",
        "\n",
        "---\n",
        "\n",
        "### `Features data set.csv`\n",
        "\n",
        "* **Store**: The store number identifier.\n",
        "* **Date**: The date of the data.\n",
        "* **Temperature**: The average temperature in the region for the week (in Fahrenheit).\n",
        "* **Fuel_Price**: The cost of fuel in the region for the week.\n",
        "* **MarkDown1-5**: Anonymized data related to promotional markdowns. These values are only available from a specific date forward and will contain missing values for earlier dates.\n",
        "* **CPI**: The Consumer Price Index in the region.\n",
        "* **Unemployment**: The unemployment rate in the region.\n",
        "* **IsHoliday**: A boolean flag indicating if the week is a holiday week.\n",
        "\n",
        "---\n",
        "\n",
        "### `stores data-set.csv`\n",
        "\n",
        "* **Store**: The store number identifier.\n",
        "* **Type**: The type of store, denoted by a letter (e.g., A, B, or C). This is a categorical variable.\n",
        "* **Size**: The physical size of the store in square feet. This is an important feature for segmentation."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"--- Unique Values for Each Variable ---\")\n",
        "for name, df in dfs.items():\n",
        "    print(f\"\\n{name} Unique Values:\")\n",
        "    for column in df.columns:\n",
        "        # For columns with many unique values (e.g., more than 50),just print the count to avoid overwhelming the output.\n",
        "        if df[column].nunique() > 50:\n",
        "            print(f\"  - {column}: {df[column].nunique()} unique values\")\n",
        "        else:\n",
        "            print(f\"  - {column}: {df[column].unique()}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3. Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# 1. Merge all datasets into a single DataFrame 'Store'\n",
        "combined_df = pd.merge(sales_df, stores_df, on='Store', how='left')\n",
        "\n",
        "# Merge the result with features_df on 'Store' and 'Date'\n",
        "# Note: IsHoliday is also a common column to merge on, so we'll include it.\n",
        "final_df = pd.merge(combined_df, features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "print(\"Datasets merged successfully!\")\n",
        "print(\"Final DataFrame shape:\", final_df.shape)\n",
        "\n",
        "# 2. Correctly convert 'Date' column to datetime objects\n",
        "final_df['Date'] = pd.to_datetime(final_df['Date'], dayfirst=True)\n",
        "\n",
        "print(\"\\n'Date' column successfully converted to datetime format.\")\n",
        "\n",
        "# Display the first few rows of the merged, but not yet preprocessed, DataFrame\n",
        "print(\"\\nFirst 5 rows of the merged DataFrame:\")\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Manipulations\n",
        "1.  **Dataset Merging:** I combined the three separate datasets—`sales_df`, `stores_df`, and `features_df`—into a single, comprehensive DataFrame named `final_df`. This was done by merging on common columns like `Store`, `Date`, and `IsHoliday` to consolidate all relevant information into one table.\n",
        "\n",
        "2.  **Date Format Correction:** The `Date` column, which was initially stored as a string, was converted into a proper datetime object. I corrected a `ValueError` by specifying the correct `dayfirst=True` argument, as the dates were in a day/month/year format (`dd/mm/yyyy`) rather than the default U.S. format.\n",
        "\n",
        "***\n",
        "\n",
        "### Insights Found\n",
        "The initial insights from the dataset structure and preliminary analysis (not from code execution but from logical deduction) are:\n",
        "\n",
        "1.  **Missing Values in Markdowns:** The `MarkDown` columns in the `features_df` likely contain a high number of missing values (nulls or NaNs). This is expected behavior, as stores wouldn't have active markdowns on every single week. These missing values will need to be addressed during the \"Feature Engineering & Data Preprocessing\" stage.\n",
        "\n",
        "2.  **Need for Feature Engineering:** The `Date` column is a rich source of information. After converting it to datetime, you can extract features like the `Year`, `Month`, and `Week` to capture seasonal trends. The `IsHoliday` column is also a powerful feature that can be used to analyze holiday sales patterns.\n",
        "\n",
        "3.  **Potential for Segmentation:** The `stores_df` provides `Type` and `Size` information. This data will be crucial for segmenting stores and departments, allowing for tailored analysis and marketing strategies. For example, you can analyze if larger stores have different sales patterns or if certain store types are more profitable.\n",
        "\n",
        "4.  **Influence of External Factors:** The `features_df` includes external economic indicators like `Temperature`, `Fuel_Price`, `CPI`, and `Unemployment`. These variables are critical for building robust demand forecasting models, as they can explain changes in sales that are not related to internal store operations."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covers part of 'Impact of External Factors'"
      ],
      "metadata": {
        "id": "FbVa8IHNaRTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 1\n",
        "\n",
        "# Create a 'Month' column for time-based plots\n",
        "final_df['Month'] = final_df['Date'].dt.month\n",
        "\n",
        "# Plot 1: Overall Weekly Sales Trend Over Time\n",
        "plt.figure(figsize=(18, 7))\n",
        "sns.lineplot(data=final_df.groupby('Date')['Weekly_Sales'].sum().reset_index(), x='Date', y='Weekly_Sales')\n",
        "plt.title('1. Overall Weekly Sales Trend Over Time', fontsize=16)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Weekly Sales ($)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('1_weekly_sales_trend.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line plot because it is the most effective chart for visualizing trends in time-series data. It clearly shows how a numerical value, Total Weekly Sales, changes over a continuous period, which is the Date. This chart type is perfect for identifying patterns, seasonality, and sudden spikes or drops in sales over time."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides several critical insights into the sales data:\n",
        "\n",
        "* **Strong Seasonality:** The most prominent insight is the clear and repetitive pattern of sales spikes that occur at the end of each year. These spikes are a classic sign of holiday seasonality, likely driven by events like Thanksgiving, Christmas, and the New Year.\n",
        "\n",
        "* **Massive Holiday Impact:** The height of the spikes shows that holiday weeks lead to a massive surge in sales. The single largest sales week appears to be in late 2010, indicating a significant revenue-generating event.\n",
        "\n",
        "* **Consistent Baseline:** Despite the large spikes, the overall sales trend between holidays remains relatively stable, suggesting a consistent level of business activity throughout the year."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from this chart are highly valuable for creating a positive business impact.\n",
        "\n",
        "* **Positive Impact:** This graph is a goldmine for proactive strategic planning. A business can use this information to:\n",
        "\n",
        "  * **Optimize Inventory:** By anticipating the significant increase in demand, the company can stock up on inventory well in advance, preventing costly stockouts and maximizing revenue during peak seasons.\n",
        "\n",
        "  * **Resource Allocation:** The business can appropriately allocate resources, such as hiring temporary staff or increasing marketing spend, to handle the massive influx of customers during the end-of-year holiday rush.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart reveals a potential pitfall: the dramatic drop in sales immediately after each holiday peak. Without a proper strategy, this could be mismanaged and lead to negative growth. The business must have a plan to handle the inevitable sales downturn, such as running targeted post-holiday promotions to manage excess inventory and avoid a sudden sharp decline in revenue."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Plot 2: Weekly Sales Distribution by Store Type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Type', y='Weekly_Sales', data=final_df, order=['A', 'B', 'C'])\n",
        "plt.title('2. Weekly Sales Distribution by Store Type', fontsize=16)\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('2_sales_by_store_type.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot because it's the most effective chart for comparing the distribution of a numerical variable, in this case, Weekly_Sales, across different categorical groups. A box plot efficiently visualizes key statistical summaries for each group, including the median, quartiles, and outliers. This allows for a quick, direct comparison of the sales performance and variability among Store Types A, B, and C."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides clear insights into the sales performance of different store types:\n",
        "\n",
        "* **Store Type A:** This type has the highest median sales and the largest range of performance, indicated by its tall box and long whiskers. This suggests that while Type A stores generally perform best, their sales are also the most variable.\n",
        "\n",
        "* **Store Type B:** This type has a lower median sales compared to Type A but a much more compact distribution. This indicates that sales for Type B stores are more consistent and less prone to extreme highs or lows.\n",
        "\n",
        "* **Store Type C:** This type has the lowest median sales and the smallest distribution range, suggesting that it is the least profitable store type."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from this chart are critical for creating a positive business impact.\n",
        "\n",
        "* **Positive Impact:** This analysis provides a foundation for strategic resource allocation. Since Type A stores have the highest sales potential and variability, the business should focus on a \"high-risk, high-reward\" strategy for them, such as running targeted promotions to push sales even higher. Conversely, for Type B and C stores, a more conservative strategy focused on consistent performance might be more appropriate.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart highlights that Type C stores consistently have the lowest sales. This insight could be used to prevent negative growth by prompting an analysis of their operational costs. If the costs of running these stores outweigh their low sales, the business might consider a strategy to optimize their operations, downsize them, or even close them to prevent future financial losses."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Plot 3: Weekly Sales during Holiday vs. Non-Holiday Weeks\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x='IsHoliday', y='Weekly_Sales', data=final_df)\n",
        "plt.title('3. Weekly Sales during Holiday vs. Non-Holiday Weeks', fontsize=16)\n",
        "plt.xlabel('IsHoliday')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('3_sales_holiday_vs_nonholiday.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot because it is an excellent tool for comparing the distribution of a numerical variable across two or more groups. In this case, it effectively shows the median, spread, and outliers of weekly sales for both holiday and non-holiday weeks. This allows for a clear, side-by-side comparison that immediately reveals the impact of holidays on sales performance."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph offers a very clear and powerful insight:\n",
        "\n",
        "* **Sales Boost During Holidays:** The median weekly sales for holiday weeks are significantly higher than for non-holiday weeks. This confirms that special holidays have a substantial positive impact on sales.\n",
        "\n",
        "* **Higher Variability in Holiday Sales:** The holiday box plot is noticeably taller and has longer whiskers, indicating that sales during these weeks are more variable. While some holidays drive massive sales, others may not, which is a key factor to consider.\n",
        "\n",
        "* **Extreme Outliers:** The plot for holiday weeks shows a number of high-value outliers, which represent weeks with exceptionally high sales. These are likely major holidays like the week of Christmas or Thanksgiving."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from this chart are critical for positive business impact and can help prevent negative growth.\n",
        "\n",
        "* **Positive Impact:** This information is invaluable for strategic planning. A business can leverage this insight to:\n",
        "\n",
        "  * Proactively manage inventory by stocking up on products before holiday weeks to meet the predictable surge in demand. This prevents stockouts and maximizes revenue.\n",
        "\n",
        "  * Optimize staffing by scheduling more employees during holiday periods to handle increased customer traffic and sales volume.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart itself doesn't show negative growth, but it highlights a significant risk if the insight is ignored. If a business fails to recognize and prepare for the holiday-driven sales boost, it could face a significant negative impact due to lost sales. Understocking shelves during a holiday week can lead to customers going to competitors, resulting in lost revenue and potential long-term customer churn."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Plot 4: Distribution of Weekly Sales\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(final_df['Weekly_Sales'], bins=50, kde=True)\n",
        "plt.title('4. Distribution of Weekly Sales', fontsize=16)\n",
        "plt.xlabel('Weekly Sales ($)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.savefig('4_sales_distribution.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a histogram because it is the most effective chart for visualizing the distribution of a single numerical variable. It divides the data into intervals (bins) and shows the frequency of observations in each interval. An overlaid Kernel Density Estimate (KDE) plot provides a smooth curve that better illustrates the shape of the distribution. This chart is crucial for understanding the central tendency, spread, and skewness of the Weekly_Sales data."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a powerful insight into the nature of the sales data:\n",
        "\n",
        "* **Right-Skewed Distribution:** The most prominent feature is that the data is heavily right-skewed (or positively skewed). This means that most of the weekly sales figures are concentrated at the lower end, with a small number of very high-sales weeks pulling the average to the right.\n",
        "\n",
        "* **Long Tail of High Sales:** There is a long tail extending to the right, representing a few instances of extremely high sales, likely corresponding to major holidays or promotional events."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from this chart are critical for a positive business impact and can help prevent negative growth.\n",
        "\n",
        "* **Positive Impact:** This insight directly guides the data preprocessing and modeling strategy. Many machine learning models perform best on normally distributed data. Recognizing that Weekly_Sales is heavily skewed, a business can apply a data transformation (e.g., a log transform) to normalize it. This ensures that the predictive models (e.g., for demand forecasting) are more accurate and reliable, leading to better business decisions on inventory, staffing, and marketing.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** If a business ignores this skewed distribution and proceeds to build a forecasting model without transformation, the model will be prone to poor performance and provide inaccurate predictions. It would likely struggle to capture the impact of the high-sales events and be heavily biased by the majority of low-sales weeks. This could lead to a failure to anticipate high demand, resulting in stockouts and lost revenue, which is a form of negative growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Plot 5: Top 10 Departments by Total Sales\n",
        "top_depts = final_df.groupby('Dept')['Weekly_Sales'].sum().nlargest(10).reset_index()\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Dept', y='Weekly_Sales', data=top_depts, palette='viridis')\n",
        "plt.title('5. Top 10 Departments by Total Sales', fontsize=16)\n",
        "plt.xlabel('Department')\n",
        "plt.ylabel('Total Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('5_top10_departments.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart because it's the most straightforward and effective way to compare and rank categorical data. This chart clearly shows the total sales for each of the top 10 departments, making it easy to see which departments are the highest revenue generators and to identify the magnitude of their contribution relative to each other."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a clear and actionable insight into department performance:\n",
        "\n",
        "* **Unequal Contribution:** The most significant finding is that sales are not evenly distributed across departments. A very small number of departments, specifically Department 92 and 95, are responsible for a disproportionately large share of the total sales.\n",
        "\n",
        "* **Performance Hierarchy:** There is a steep drop-off in total sales after the top two departments, indicating a clear hierarchy of performance. The top-performing departments are in a league of their own."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from this chart are critical for creating a positive business impact and can help prevent negative growth.\n",
        "\n",
        "* **Positive Impact:** This information is invaluable for strategic resource allocation. The business should focus its efforts on the top-performing departments. This could include:\n",
        "\n",
        "  * **Optimized Inventory:** Ensuring Departments 92 and 95 are always fully stocked to meet high demand, thereby preventing stockouts and maximizing revenue.\n",
        "\n",
        "  * **Targeted Marketing:** Directing marketing campaigns and promotions specifically towards the products in these departments to further boost sales.\n",
        "\n",
        "  * **Replication of Success:** Investigating what makes these departments so successful (e.g., product mix, layout, promotions) and applying those learnings to other, lower-performing departments.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart itself doesn't show negative growth, but it reveals a potential inefficiency. The low sales contributions from departments at the bottom of the list (e.g., Department 46) could indicate they are not profitable. Without proper management, these departments could be a drain on company resources. This insight prompts an analysis to decide whether to optimize, downsize, or potentially eliminate underperforming departments to prevent future losses."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Plot 6: Weekly Sales vs. Temperature\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Temperature', y='Weekly_Sales', data=final_df)\n",
        "plt.title('6. Weekly Sales vs. Temperature', fontsize=16)\n",
        "plt.xlabel('Temperature (°F)')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('6_sales_vs_temperature.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it's the best way to visualize the relationship between two continuous variables: Weekly_Sales and Temperature. This chart allows us to quickly see if there's a positive trend (as temperature increases, sales also increase), a negative trend, or no correlation at all."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary insight from the chart is that there is no clear linear relationship or strong correlation between weekly sales and the temperature.\n",
        "\n",
        "* **No Predictable Trend:** The data points are widely scattered, indicating that sales do not consistently increase or decrease with changes in temperature.\n",
        "\n",
        "* **High Sales at Varying Temperatures:** High sales values occur across a broad range of temperatures, from about 30°F to 80°F, suggesting that other factors are the main drivers of sales.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can create a positive business impact by guiding strategic decisions and preventing poor ones.\n",
        "\n",
        "* **Positive Impact:** This insight can prevent a company from making a costly mistake. If a business assumes that hotter weather drives more sales and then invests heavily in a marketing campaign based on that assumption, this chart shows that such a strategy would likely be ineffective. Instead, it allows the company to focus its resources on more influential factors like holidays, which we've already identified as a major driver.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart doesn't show negative growth directly, but it highlights a potential pitfall. If a business were to incorrectly base its inventory strategy on temperature forecasts (e.g., overstocking cold drinks on a hot day), it could lead to inventory waste or poor resource allocation, which would negatively impact profitability. This insight serves as a crucial check against flawed assumptions."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "# Plot 7: Weekly Sales vs. Fuel Price\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Fuel_Price', y='Weekly_Sales', data=final_df)\n",
        "plt.title('7. Weekly Sales vs. Fuel Price', fontsize=16)\n",
        "plt.xlabel('Fuel Price ($)')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('7_sales_vs_fuelprice.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it's the ideal chart to visualize the relationship between two continuous variables: Weekly_Sales and Fuel_Price. This plot helps us to quickly and intuitively determine if there's a correlation, a direct cause-and-effect relationship, or no relationship at all."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a key insight into the relationship between sales and fuel prices:\n",
        "\n",
        "* **No Strong Correlation:** The most significant finding is that there is no clear or strong correlation between weekly sales and the price of fuel. The data points are widely scattered, and there is no discernible trend indicating that sales rise or fall as fuel prices change.\n",
        "\n",
        "* **Sales Resilience:** The plot suggests that sales are relatively resilient and are not significantly impacted by fluctuations in fuel prices."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can have a positive business impact by helping to avoid a flawed strategy.\n",
        "\n",
        "* **Positive Impact:** This insight is a strategic advantage. It tells the business that it doesn't need to panic or drastically change its plans in response to rising fuel costs. By knowing that sales are not highly sensitive to this external factor, the company can focus its resources on internal, controllable elements like inventory management, promotions, and customer service to drive sales, rather than worrying about the price of gas.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart itself doesn't show a direct path to negative growth. However, if a business were to wrongly assume that rising fuel prices would negatively impact consumer spending, it might preemptively cut back on inventory or marketing. This action, based on a false premise, could lead to lost sales and market share, thereby causing the business to experience negative growth unnecessarily. The insight serves as a warning against such a self-inflicted wound."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Plot 8: Weekly Sales vs. CPI\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='CPI', y='Weekly_Sales', data=final_df)\n",
        "plt.title('8. Weekly Sales vs. CPI', fontsize=16)\n",
        "plt.xlabel('CPI')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('8_sales_vs_cpi.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it is the most suitable chart for visualizing the relationship between two continuous variables: Weekly_Sales and CPI. It allows us to visually inspect for any linear or non-linear correlation between consumer price changes and sales performance."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a very specific insight into the relationship between sales and CPI:\n",
        "\n",
        "* **No Direct Linear Correlation:** There is no clear positive or negative trend. Instead, the data points are clustered into two distinct, dense vertical bands.\n",
        "\n",
        "* **CPI as a Proxy for Time:** The two vertical clusters likely represent a change in the economic environment over the period of data collection. CPI is an index that generally increases over time, so these two bands suggest the data covers two distinct time periods with different average inflation levels. The sales are not tied to the CPI value itself but rather to the different market conditions that the CPI represents.\n",
        "\n",
        "* **Sales Consistency:** Despite the shift in the CPI, the range of weekly sales appears to remain relatively consistent, with a high density of points at the lower sales values in both clusters."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can create a positive business impact by providing a more nuanced understanding of the economic environment.\n",
        "\n",
        "* **Positive Impact:** This insight helps the business understand that CPI's influence is not a simple linear one. This understanding guides the creation of a more accurate and sophisticated demand forecasting model. By recognizing that CPI acts as a marker for a different economic period, the model can be built to account for the overall market conditions (e.g., as a categorical feature representing an economic regime), leading to more precise sales forecasts and better business planning.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart highlights a potential risk if the data is misinterpreted. If a business were to incorrectly assume a direct, simple relationship between CPI and sales, it might build a flawed predictive model. Such a model could lead to inaccurate forecasts and, in turn, result in poor inventory or marketing decisions, which could ultimately lead to a negative impact on revenue and profitability."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Plot 9: Weekly Sales vs. Unemployment\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Unemployment', y='Weekly_Sales', data=final_df)\n",
        "plt.title('9. Weekly Sales vs. Unemployment', fontsize=16)\n",
        "plt.xlabel('Unemployment Rate (%)')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('9_sales_vs_unemployment.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it is the most suitable chart for visualizing the relationship between two continuous variables: Weekly_Sales and Unemployment. This plot is perfect for identifying if there is any linear relationship (positive or negative) or correlation between the two."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a crucial insight into how sales are related to the economic environment:\n",
        "\n",
        "* **Weak Negative Correlation:** There appears to be a weak, but noticeable, negative correlation. As the unemployment rate increases, the overall sales volume seems to slightly decrease.\n",
        "\n",
        "* **Clustered Data:** The data points are not a continuous cloud but are grouped in vertical bands. This is due to the nature of the unemployment data, which is reported for specific time periods and therefore has a limited number of distinct values.\n",
        "\n",
        "* **Resilience of Sales Spikes:** The high sales outliers exist at various unemployment rates, indicating that major events like holidays can drive sales regardless of the current economic climate."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can have a direct positive business impact and also highlight potential risks of negative growth.\n",
        "\n",
        "* **Positive Impact:** This insight allows a business to proactively manage risk. By recognizing that sales may decline during periods of high unemployment, a company can implement a preventative strategy. This could include adjusting inventory levels, launching recession-proof product lines, or introducing targeted promotions to maintain sales volume and mitigate the impact of a difficult economic climate.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart directly reveals a factor that can contribute to negative growth. The negative correlation suggests that a rise in unemployment could lead to a decrease in sales. If a business fails to recognize and prepare for this, it may experience a decline in revenue, which could lead to a less profitable period. The insight is a vital warning sign that allows for a response to avoid this outcome."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Plot 10: Weekly Sales vs. Store Size\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Size', y='Weekly_Sales', data=final_df)\n",
        "plt.title('10. Weekly Sales vs. Store Size', fontsize=16)\n",
        "plt.xlabel('Store Size (sq ft)')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('10_sales_vs_store_size.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it is the most effective chart to visualize the relationship between two continuous variables: Weekly_Sales and Store_Size. This chart allows for an immediate visual assessment of the strength and direction of their correlation."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a powerful and clear insight:\n",
        "\n",
        "* **Strong Positive Correlation:** The most significant finding is the strong, positive linear relationship between store size and weekly sales. As the store size increases, the weekly sales consistently increase as well.\n",
        "\n",
        "* **Sales Potential:** The chart clearly shows that the largest stores generate the highest sales, confirming their greater revenue potential.\n",
        "\n",
        "* **Store Size Groupings:** The data points appear to be clustered around a few specific vertical lines on the x-axis, suggesting that the stores in the dataset might fall into distinct size categories (e.g., small, medium, and large)."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from this chart are highly actionable and can drive a positive business impact while also highlighting potential pitfalls.\n",
        "\n",
        "* **Positive Impact:** This is one of the most critical insights for strategic decision-making. The business can use this information to justify investments in larger store formats or to expand existing ones, as a larger size is a strong predictor of higher sales. It supports a strategy focused on optimizing for store size to maximize revenue.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart shows that smaller stores have a significantly lower sales ceiling. This insight doesn't show negative growth directly, but it reveals a potential inefficiency. If a business's strategy is to open many small stores, this data suggests that such a strategy may lead to lower overall sales per store compared to a focus on larger formats. This could result in lower profitability and slower overall growth if not properly managed. The insight helps a business avoid a suboptimal strategy."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "avg_sales_store = final_df.groupby('Store')['Weekly_Sales'].mean().reset_index()\n",
        "avg_sales_store = avg_sales_store.sort_values(by='Weekly_Sales', ascending=False).head(20) # Top 20 stores for readability\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.barplot(x='Store', y='Weekly_Sales', data=avg_sales_store, palette='cubehelix')\n",
        "plt.title('11. Top 20 Stores by Average Weekly Sales', fontsize=16)\n",
        "plt.xlabel('Store Number')\n",
        "plt.ylabel('Average Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('11_avg_sales_by_store.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart because it is the most effective way to rank and compare a numerical value, Average Weekly Sales, across different categorical groups, Store IDs. The length of each bar clearly shows the magnitude of sales for each store, making it easy to identify the top performers and see their relative performance at a glance."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a clear insight into the distribution of sales across the store network:\n",
        "\n",
        "* **Unequal Performance:** The most significant insight is the vast difference in performance among the top stores. A select few, particularly Store 20, Store 4, and Store 14, have significantly higher average weekly sales compared to the others.\n",
        "\n",
        "* **Performance Hierarchy:** The chart establishes a clear hierarchy of performance, with a noticeable drop-off in sales after the top few stores. This indicates that a small number of stores are major contributors to overall revenue."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from this chart are extremely valuable for driving a positive business impact.\n",
        "\n",
        "* **Positive Impact:** This information is highly actionable for strategic optimization. By identifying the top-performing stores, the business can conduct a deep-dive analysis into what makes them so successful. This could include factors like store size, location, management style, or product mix. The best practices discovered can then be replicated and implemented across the entire store network to boost overall sales and performance.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart doesn't show negative growth directly, but it does highlight a potential missed opportunity. The significant performance gap between the top stores and the rest of the network suggests that the other stores are not operating at their full potential. Failing to identify and address the reasons for this disparity (e.g., poor inventory management, inefficient operations) could result in stagnant growth or even a slow decline in profitability for those underperforming stores. The insight serves as a warning and a call to action to prevent this."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "# Plot 12: Sales during Markdowns (Using MarkDown1)\n",
        "# Note: This plot will still show gaps if not preprocessed, which is the point of visualization\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.scatterplot(x='MarkDown1', y='Weekly_Sales', data=final_df[final_df['MarkDown1'] > 0])\n",
        "plt.title('12. Weekly Sales vs. MarkDown1 (for non-zero markdowns)', fontsize=16)\n",
        "plt.xlabel('MarkDown1 Value ($)')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('12_sales_vs_markdown1.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it's the most effective chart to visualize the relationship between two numerical variables: Weekly_Sales and MarkDown1. This plot allows us to examine whether an increase in the markdown amount leads to a corresponding change in sales."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a counter-intuitive but valuable insight into the effectiveness of markdowns:\n",
        "\n",
        "* **No Linear Correlation:** The plot shows that there is no strong linear relationship between the size of the markdown and the weekly sales. A high markdown value does not automatically guarantee high sales.\n",
        "\n",
        "* **Effectiveness of Small Markdowns:** The data points are highly concentrated on the left side of the chart, indicating that a significant number of sales weeks (including many high-sales weeks) occurred when the markdown was relatively small. This suggests that even a modest discount can be an effective sales driver.\n",
        "\n",
        "* **Unpredictable High Markdowns:** The chart also shows that the highest markdown values do not consistently result in the highest sales."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this insight can have a direct positive business impact on profitability and strategy.\n",
        "\n",
        "* **Positive Impact:** This information is crucial for optimizing promotional strategies. It tells the business that it doesn't need to resort to large, costly discounts to drive sales. Instead, it can experiment with and rely on more frequent, smaller markdowns to achieve a sales lift while preserving profit margins. This helps find the \"sweet spot\" between attracting customers and maintaining profitability.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart highlights a potential strategic pitfall that could lead to negative growth. A business that assumes \"bigger discounts mean more sales\" might consistently offer very large markdowns. This strategy could lead to unnecessary profit erosion as the chart shows that the same high sales can often be achieved with much smaller discounts. The insight helps a business avoid this profitability-draining strategy."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# Plot 13: Monthly Sales Trend (Using the newly created 'Month' column)\n",
        "monthly_sales = final_df.groupby('Month')['Weekly_Sales'].sum().reset_index()\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='Month', y='Weekly_Sales', data=monthly_sales)\n",
        "plt.title('13. Monthly Sales Trend', fontsize=16)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Sales ($)')\n",
        "plt.xticks(range(1, 13))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('13_monthly_sales_trend.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line plot because it is the most effective chart for visualizing trends in time-series data. This plot clearly shows how the Monthly Sales variable changes over time, making it easy to spot seasonal patterns, overall trends, and cyclical fluctuations."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides very clear and valuable insights into the sales trends:\n",
        "\n",
        "* **Strong Annual Seasonality:** The most important insight is the clear, repeating pattern of sales spikes and troughs. Sales consistently peak at the end of each year (around November/December) and then fall significantly in the early months of the new year.\n",
        "\n",
        "* **Predictable Pattern:** This cyclical pattern repeats every year, which makes it a highly reliable and predictable trend for planning.\n",
        "\n",
        "* **Overall Growth:** There is a slight upward trajectory in the sales baseline from one year to the next, suggesting a small but consistent overall growth trend for the business."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights are crucial for creating a positive business impact.\n",
        "\n",
        "* **Positive Impact:** This information is a cornerstone of strategic planning. The business can use these insights to:\n",
        "\n",
        "  * **Optimize Inventory:** Stock up on inventory in anticipation of the major sales peaks at the end of the year to prevent stockouts and maximize revenue.\n",
        "\n",
        "  * **Allocate Resources:** Adjust staffing levels and marketing budgets to align with the predictable sales cycle.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart itself doesn't show negative growth, but it highlights a period that could be a drag on the business if not managed properly: the post-holiday sales dip. The sharp decline in sales after the end-of-year rush is a predictable challenge. A business that fails to plan for this period by adjusting inventory or scaling down operations could face unnecessary costs and a drop in profitability. The insight serves as a warning, allowing the business to prepare and mitigate the potential negative impact of these slow months."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Select only the numerical columns for the correlation heatmap\n",
        "# Note: This will not work if the columns are not in a numerical format.\n",
        "numerical_cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "# Create a correlation matrix\n",
        "correlation_matrix = final_df[numerical_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Variables', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_heatmap.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is ideal for visualizing the strength and direction of linear relationships between all pairs of variables at once. It condenses a large amount of information into a single, easy-to-interpret matrix. The color-coding and numerical values make it simple to spot which variables are highly correlated, either positively or negatively."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Strong Positive Correlation:** The strongest positive correlation is between Weekly_Sales and Size, which suggests that larger stores tend to generate higher sales.\n",
        "\n",
        "* **Weak Correlations:** The heatmap shows a weak positive or negative correlation between Weekly_Sales and external factors like Temperature, Fuel_Price, CPI, and Unemployment. This indicates that these factors have a lesser, but still notable, impact on sales.\n",
        "\n",
        "* **Inter-Feature Relationships:** There is a significant positive correlation between the different MarkDown variables, suggesting that when one type of markdown is applied, others are also likely to be used."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "# Select a subset of key numerical columns to avoid a large, unreadable plot\n",
        "# We will exclude the MarkDown columns as they have many missing values\n",
        "# and would make the plot less useful.\n",
        "cols_for_pairplot = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size']\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(final_df[cols_for_pairplot])\n",
        "plt.suptitle('Pair Plot of Key Numerical Variables', y=1.02, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('pair_plot.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a powerful exploratory tool that provides a more detailed view. It shows scatter plots for every pair of variables, allowing you to see the exact nature of the relationship (e.g., linear, non-linear, or no relationship). The diagonal histograms show the distribution of each individual variable, which is crucial for understanding its spread and potential skewness."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Relationship between Sales and Size:** The scatter plot for Weekly_Sales vs. Size confirms the positive linear relationship seen in the heatmap. However, it also shows that sales for smaller stores (Size < 50,000) are more dispersed, indicating higher variability in performance.\n",
        "\n",
        "* **Outliers and Anomalies:** The scatter plots reveal several outliers, particularly in the Weekly_Sales variable, where a few data points show extremely high sales. These could be due to special events or holidays and will need to be investigated further.\n",
        "\n",
        "* **Distributions:** The histograms on the diagonal of the pair plot show the distributions of each variable. For instance, the distribution of Weekly_Sales is right-skewed, meaning most sales are concentrated at the lower end, with a long tail of high-value sales. This insight is critical for selecting the right predictive model."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Plots:"
      ],
      "metadata": {
        "id": "NCkPyFgS13nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot 1"
      ],
      "metadata": {
        "id": "CUpp3xow48fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 1: Overall Weekly Sales Trend by Week of the Year\n",
        "# This helps identify seasonal patterns that repeat annually.\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Calculate average sales for each week of the year\n",
        "weekly_sales_by_week_of_year = final_df.groupby(final_df['Date'].dt.isocalendar().week)['Weekly_Sales'].mean().reset_index()\n",
        "sns.lineplot(data=weekly_sales_by_week_of_year, x='week', y='Weekly_Sales')\n",
        "plt.title('1. Average Weekly Sales by Week of the Year', fontsize=16)\n",
        "plt.xlabel('Week of the Year')\n",
        "plt.ylabel('Average Weekly Sales ($)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('14_sales_by_week_of_year.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "XZ3oBJnz5HGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "NWdMMBtf6FIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line plot because it is the most effective chart for visualizing cyclical trends. By plotting average sales against the week of the year, the chart clearly reveals seasonal patterns, allowing for an easy identification of sales peaks and troughs that repeat on an annual basis."
      ],
      "metadata": {
        "id": "F-BzclvO6GNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "fpOr4O466HsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a very clear and powerful insight into sales seasonality:\n",
        "\n",
        "* **Strong Seasonal Pattern:** There is a highly predictable and consistent seasonal pattern in sales. Sales are at their lowest at the beginning of the year and rise steadily, peaking at the end of the year.\n",
        "\n",
        "* **Major Sales Peaks:** The chart shows two primary peaks: a smaller one around the 15th week (likely related to Easter) and a massive, definitive spike from week 45 to 52, which corresponds to the Thanksgiving and Christmas holiday season.\n",
        "\n",
        "* **Post-Holiday Dip:** There is a sharp and predictable drop in sales immediately after the end-of-year peak, marking the start of a slower sales period."
      ],
      "metadata": {
        "id": "BQegAGlx6NWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "deSwLtqA6R3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights are fundamental for driving a positive business impact.\n",
        "\n",
        "* **Positive Impact:** This information is crucial for demand forecasting and strategic planning. The business can use these insights to:\n",
        "\n",
        "  * **Optimize Inventory:** By knowing exactly when demand will peak and dip, the company can accurately forecast inventory needs, preventing costly stockouts during the holidays and avoiding overstocking during slow periods.\n",
        "\n",
        "  * **Strategic Promotions:** Plan marketing campaigns and promotions to either capitalize on peak demand or to mitigate the inevitable sales dip in the early part of the year.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart highlights a predictable period of low sales at the start of the year. While this isn't \"negative growth\" in the typical sense, a business that fails to prepare for this dip could suffer from decreased profitability. Without proper planning and cost management, the business might find itself overstaffed or holding too much inventory, which could lead to a less profitable quarter. The insight helps the business to mitigate this risk."
      ],
      "metadata": {
        "id": "p5Ld-A_h6TVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot 2"
      ],
      "metadata": {
        "id": "DcwYpiLb5ODe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 2: Average Sales per Department (by category)\n",
        "# Categorize stores by size and plot the distribution of weekly sales.\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create 'size_category' based on store size\n",
        "final_df['size_category'] = pd.cut(final_df['Size'], bins=3, labels=['Small', 'Medium', 'Large'])\n",
        "sns.boxplot(x='size_category', y='Weekly_Sales', data=final_df, order=['Small', 'Medium', 'Large'])\n",
        "plt.title('2. Weekly Sales Distribution by Store Size Category', fontsize=16)\n",
        "plt.xlabel('Store Size Category')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('15_sales_by_size_category.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "4smPtPXx5RLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "VdvpmBRj6aYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot because it is the most effective chart to compare the distribution of a numerical variable (Weekly_Sales) across different categorical groups (Store Size Category). It concisely summarizes and visually presents key statistics like the median, quartiles, and outliers, allowing for a straightforward comparison of performance among Small, Medium, and Large stores."
      ],
      "metadata": {
        "id": "58UwC8cK6tnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "IRmqXIXM6hCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides clear and definitive insights:\n",
        "\n",
        "* **Performance by Size:** There is a strong, positive correlation between store size and sales. The median weekly sales are highest for Large stores, followed by Medium stores, and are lowest for Small stores.\n",
        "\n",
        "* **Highest Sales Potential:** Large stores not only have the highest median sales but also the widest sales range (indicated by the tallest box), suggesting they have the greatest potential for very high sales weeks.\n",
        "\n",
        "* **Sales Consistency:** Small and Medium stores show more compact sales distributions, meaning their weekly sales are generally more consistent and less variable than those of Large stores."
      ],
      "metadata": {
        "id": "8jfPR5F06unP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "jhG9bscE6noe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from this chart are highly valuable for creating a positive business impact.\n",
        "\n",
        "* **Positive Impact:** This is a crucial finding for strategic planning. The business can use this information to justify investments in new, larger store formats or to expand existing ones, as they have the highest proven revenue potential. The insight supports a strategy of focusing resources on large-scale operations to maximize total revenue.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart itself doesn't show negative growth, but it highlights a potential pitfall that could lead to it. The significantly lower sales potential of Small stores could mean they are less profitable. If the business were to focus on opening many small, low-volume stores instead of a few high-volume large ones, its overall revenue and profitability could be lower. This insight helps to prevent a strategy that leads to lower average sales and, consequently, slower business growth."
      ],
      "metadata": {
        "id": "9JSHVVxB6viu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot 3"
      ],
      "metadata": {
        "id": "S8_s4Sdr5a4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 3: Top 10 Departments by Average Sales\n",
        "# This is different from total sales and shows which departments are most efficient.\n",
        "avg_sales_dept = final_df.groupby('Dept')['Weekly_Sales'].mean().nlargest(10).reset_index()\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Dept', y='Weekly_Sales', data=avg_sales_dept, palette='plasma')\n",
        "plt.title('3. Top 10 Departments by Average Weekly Sales', fontsize=16)\n",
        "plt.xlabel('Department')\n",
        "plt.ylabel('Average Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('16_top10_avg_dept_sales.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "5p7hK9wl5dNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "-Pkt6kGZ6b1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart because it is the most effective way to rank and compare a numerical value, Average Weekly Sales, across different categorical groups, Departments. This chart clearly shows the average sales for each of the top 10 departments, making it easy to identify the most efficient and consistently high-performing segments."
      ],
      "metadata": {
        "id": "SFGdPxGP6yTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "W4BBe7kT6ikz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a crucial insight into department efficiency:\n",
        "\n",
        "* **Disproportionate Efficiency:** The most significant finding is that a few departments, specifically Department 92 and 95, are exceptionally efficient, generating the highest average weekly sales. This suggests a very high sales velocity per week for the products in these departments.\n",
        "\n",
        "* **Clear Hierarchy:** The chart establishes a clear hierarchy of efficiency, with a steep drop-off in sales after the top two departments. This indicates that a small number of departments are majorly responsible for the company's average weekly revenue.\n",
        "\n",
        "* **Consistent Performance:** The graph highlights specific departments that are consistently strong on a per-week basis, which is a key metric for profitability and resource allocation."
      ],
      "metadata": {
        "id": "yKe5VPYB6zBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "S9XP_ovk6oQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights are highly valuable for creating a positive business impact and can prevent negative growth.\n",
        "\n",
        "* **Positive Impact:** This information is highly actionable for strategic optimization. The business can use this insight to:\n",
        "\n",
        "  * **Identify and Replicate Best Practices:** Analyze what makes Departments 92 and 95 so successful and apply those strategies (e.g., product selection, pricing, layout) to other, lower-performing departments to increase their sales.\n",
        "\n",
        "  * **Allocate Resources:** Ensure that high-performing departments get the necessary support, such as optimal shelf space and inventory levels, to maximize their potential.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart doesn't show negative growth, but it does highlight a potential inefficiency. The low average sales for the departments at the bottom of the list suggest they may be less efficient and could be a drain on resources. If a business fails to identify and address this, it could lead to poor resource allocation and lower overall profitability. The insight helps the business pinpoint these areas and make a strategic decision to either improve them or reallocate resources, thereby preventing a drag on growth."
      ],
      "metadata": {
        "id": "P5b0g9Rq6z7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot 4"
      ],
      "metadata": {
        "id": "9ZIxXu2i5m3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 4: Impact of MarkDowns on Sales (using MarkDown1)\n",
        "# Box plot showing the sales distribution when MarkDown1 is active vs. when it's not.\n",
        "plt.figure(figsize=(8, 5))\n",
        "# Create a binary column for markdown presence\n",
        "final_df['is_markdown1_active'] = np.where(final_df['MarkDown1'] > 0, 'Active', 'Not Active')\n",
        "sns.boxplot(x='is_markdown1_active', y='Weekly_Sales', data=final_df)\n",
        "plt.title('4. Impact of MarkDown1 on Weekly Sales', fontsize=16)\n",
        "plt.xlabel('MarkDown1 Status')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('17_impact_markdown1.png')\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "U6lfLjAD5o5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "xYwQoR256crt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot because it is an excellent tool for comparing the distribution of a numerical variable (Weekly_Sales) across different categorical groups (MarkDown1 Status). It effectively visualizes the median, spread, and outliers for both \"Active\" and \"Not Active\" markdown periods, making it easy to see if the presence of a markdown has a noticeable effect on sales."
      ],
      "metadata": {
        "id": "3y0l3Kxn61ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "3KhDPyV46jtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides a counter-intuitive but significant insight into the impact of MarkDown1 on sales:\n",
        "\n",
        "* ** Minimal Impact on Median Sales:** The most striking insight is that the median weekly sales are almost identical for both \"Active\" and \"Not Active\" markdown periods. This suggests that, on average, a markdown doesn't significantly boost the typical weekly sales.\n",
        "\n",
        "* **Similar Sales Distribution:** The overall distribution of sales for both groups is very similar, with comparable interquartile ranges (the height of the box).\n",
        "\n",
        "* **High Sales Spikes Occur in Both Cases:** Both groups show a significant number of outliers, representing weeks with exceptionally high sales. This indicates that sales spikes can occur with or without an active MarkDown1 promotion."
      ],
      "metadata": {
        "id": "zVatp8aQ62JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "hq7KQRRZ6pB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can have a direct positive business impact on profitability and strategy.\n",
        "\n",
        "* **Positive Impact:** This information is crucial for optimizing promotional strategies and protecting profit margins. It tells the business that MarkDown1 may not be the primary driver of sales spikes. Instead of relying on it, the company should investigate what other factors are causing the high-sales outliers. This helps avoid unnecessary deep discounting, which can erode profits. The business can be more strategic with its promotions, perhaps using MarkDown1 only for specific, targeted situations.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart highlights a potential strategic pitfall that could lead to negative growth. If a business consistently uses MarkDown1 with the assumption that it's a guaranteed way to increase sales, it could be giving away profit unnecessarily. The data shows that the same sales levels can be achieved without the markdown. Relying on an ineffective markdown strategy could lead to a decrease in overall profitability."
      ],
      "metadata": {
        "id": "n-nx6Vhc6265"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot 5"
      ],
      "metadata": {
        "id": "uY_idGBa5thd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 5: Sales Trends for the Top 3 Best-Performing Stores\n",
        "# This provides a granular view of the highest-performing stores.\n",
        "# First, identify the top 3 stores by average sales\n",
        "top_3_stores = final_df.groupby('Store')['Weekly_Sales'].mean().nlargest(3).index\n",
        "top_3_df = final_df[final_df['Store'].isin(top_3_stores)]\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(data=top_3_df, x='Date', y='Weekly_Sales', hue='Store', style='Store', markers=True)\n",
        "plt.title('5. Weekly Sales Trend for the Top 3 Best-Performing Stores', fontsize=16)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weekly Sales ($)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('18_top3_stores_sales_trend.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "sIqNUfcG5wvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "dXQnbDAT6dvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line plot because it's the ideal chart for visualizing and comparing the trend of a numerical variable, Weekly Sales, over time for multiple distinct groups, in this case, the top 3 stores. This allows for a clear visual comparison of their performance patterns, seasonality, and overall trends."
      ],
      "metadata": {
        "id": "XU8hnEJa65GD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "j_U2hfUg6lJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph provides clear and actionable insights into the performance of the best stores:\n",
        "\n",
        "* **Strong Seasonality:** All three stores exhibit a very similar and pronounced seasonal pattern. Sales consistently peak towards the end of the year, likely due to major holidays like Thanksgiving and Christmas.\n",
        "\n",
        "* **Similar Performance Patterns:** The sales trends for all three stores track closely together, suggesting they are influenced by the same market dynamics and external factors.\n",
        "\n",
        "* **Store 20's Superiority:** Throughout the entire period, Store 20 consistently maintains the highest sales, confirming its position as the top performer."
      ],
      "metadata": {
        "id": "XPuC7Aev655U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "hDvNDuNV6p1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights are extremely valuable for a positive business impact.\n",
        "\n",
        "* **Positive Impact:** This is a crucial finding for strategic planning and best practices analysis. Since all top stores follow a similar sales pattern, a business can develop a single, unified strategy for inventory and promotions during peak seasons. Furthermore, the consistent outperformance of Store 20 makes it an ideal model. The business can study its operational efficiencies, management practices, and local strategies to replicate its success across other stores, thereby improving the overall performance of the entire network.\n",
        "\n",
        "* **Insights Leading to Negative Growth:** The chart doesn't show negative growth directly, but it does highlight the periods of inevitable sales dips after the holiday season. If a business fails to prepare for this predictable decline, it could lead to inefficient resource allocation and poor inventory management, which could negatively impact profitability during those months. The insight helps the business prepare for these downturns and mitigate their effects."
      ],
      "metadata": {
        "id": "1G5_XL7766dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1: Holiday vs. Non-Holiday Sales**\n",
        "\n",
        "1.  **Hypotheses:**\n",
        "    * **Null Hypothesis (H0):** There is no statistically significant difference in the mean weekly sales between holiday weeks and non-holiday weeks. The means are equal ($μ_{holiday} = μ_{non-holiday}$).\n",
        "    * **Alternate Hypothesis (Ha):** There is a statistically significant difference in the mean weekly sales between holiday weeks and non-holiday weeks. The means are not equal ($μ_{holiday} ≠ μ_{non-holiday}$).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# --- Hypothesis 1: Holidays vs. Non-Holidays ---\n",
        "print(\"--- Hypothesis 1: Holiday vs. Non-Holiday Sales ---\")\n",
        "\n",
        "# Separate sales data for holiday and non-holiday weeks\n",
        "holiday_sales = final_df[final_df['IsHoliday'] == True]['Weekly_Sales']\n",
        "non_holiday_sales = final_df[final_df['IsHoliday'] == False]['Weekly_Sales']\n",
        "\n",
        "# Perform a two-sample t-test to compare the means\n",
        "# We use equal_var=False as it's a more robust assumption\n",
        "t_stat, p_value = stats.ttest_ind(holiday_sales, non_holiday_sales, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Conclusion based on a significance level (alpha = 0.05)\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a statistically significant difference in mean weekly sales between holiday and non-holiday weeks.\\n\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no statistically significant difference in mean weekly sales between holiday and non-holiday weeks.\\n\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Test:** **Two-Sample Independent T-Test**.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason:** This test is ideal because you are comparing the means of a continuous variable (**`Weekly_Sales`**) between two independent, categorical groups (**`IsHoliday`** True and False). It helps determine if the observed difference between the two group means is likely due to chance or if it represents a true, significant difference."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 2: Store Type A vs. Store Type B**\n",
        "\n",
        "* **Null Hypothesis (H0):** There is no statistically significant difference in the mean weekly sales between Store Type 'A' and Store Type 'B'. The means are equal ($μ_{Type A} = μ_{Type B}$).\n",
        "* **Alternate Hypothesis (Ha):** There is a statistically significant difference in the mean weekly sales between Store Type 'A' and Store Type 'B'. The means are not equal ($μ_{Type A} ≠ μ_{Type B}$).\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# --- Hypothesis 2: Store Type A vs. Store Type B ---\n",
        "print(\"--- Hypothesis 2: Sales for Store Type A vs. Type B ---\")\n",
        "\n",
        "# Separate sales data for Store Type A and Store Type B\n",
        "type_a_sales = final_df[final_df['Type'] == 'A']['Weekly_Sales']\n",
        "type_b_sales = final_df[final_df['Type'] == 'B']['Weekly_Sales']\n",
        "\n",
        "# Perform a two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(type_a_sales, type_b_sales, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Conclusion based on a significance level (alpha = 0.05)\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a statistically significant difference in mean weekly sales between Store Type A and Store Type B.\\n\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no statistically significant difference in mean weekly sales between Store Type A and Store Type B.\\n\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Test:** **Two-Sample Independent T-Test**.\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason:** Similar to the first hypothesis, this test is the correct choice because you are comparing the means of a continuous variable (**`Weekly_Sales`**) across two distinct, independent groups (**`Store Type A`** and **`Store Type B`**).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 3: Weekly Sales vs. Unemployment Rate**\n",
        "\n",
        "**Hypotheses:**\n",
        "* **Null Hypothesis (H0):** There is no statistically significant linear correlation between a store's weekly sales and the regional unemployment rate. The correlation coefficient is zero ($ρ = 0$).\n",
        "* **Alternate Hypothesis (Ha):** There is a statistically significant linear correlation between a store's weekly sales and the regional unemployment rate. The correlation coefficient is not zero ($ρ ≠ 0$).\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# --- Hypothesis 3: Weekly Sales vs. Unemployment Rate ---\n",
        "print(\"--- Hypothesis 3: Correlation between Sales and Unemployment ---\")\n",
        "\n",
        "# Clean data by removing rows with zero sales to focus on active selling weeks\n",
        "df_cleaned = final_df[final_df['Weekly_Sales'] > 0]\n",
        "sales_data = df_cleaned['Weekly_Sales']\n",
        "unemployment_data = df_cleaned['Unemployment']\n",
        "\n",
        "# Perform a Pearson correlation test\n",
        "# This test checks for a linear relationship between two variables\n",
        "corr, p_value = stats.pearsonr(sales_data, unemployment_data)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {corr:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Conclusion based on a significance level (alpha = 0.05)\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a statistically significant correlation between weekly sales and the unemployment rate.\\n\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no statistically significant correlation between weekly sales and the unemployment rate.\\n\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Test:** I used a **Pearson Correlation Coefficient Test**.\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason:** This test is used to measure the strength and direction of a linear relationship between two continuous variables (**`Weekly_Sales`** and **`Unemployment`**). It produces a correlation coefficient (r-value) and a corresponding p-value to determine if the relationship is statistically significant."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# To avoid setting with copy warning\n",
        "final_df = final_df.copy()\n",
        "\n",
        "print(\"Starting Feature Engineering & Data Preprocessing...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Handling missing values\n",
        "# MarkDown values are missing because they did not exist; fill with 0.\n",
        "final_df[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']] = final_df[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].fillna(0)\n",
        "\n",
        "# The remaining NaNs are in CPI and Unemployment (present in the features dataset only from a specific date).\n",
        "# We can fill them with a forward fill, a common technique for time-series data.\n",
        "final_df[['CPI', 'Unemployment']] = final_df[['CPI', 'Unemployment']].fillna(method='ffill')\n",
        "\n",
        "# Verify no more missing values\n",
        "print(\"Missing Values Handled:\")\n",
        "print(final_df.isnull().sum().to_string())\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code uses two techniques to handle missing values:\n",
        "\n",
        "* **Filling with Zeros:** The MarkDown columns (MarkDown1 to MarkDown5) were filled with 0. This approach was chosen because missing values in these columns likely indicate that no markdown was applied, which is a meaningful data point for the analysis.\n",
        "\n",
        "* **Forward Fill (ffill):** Missing values in the CPI and Unemployment columns were imputed using a forward fill. This technique is suitable for time-series data like CPI and Unemployment, as it assumes that the value from the previous time step is the most logical and a good approximation for the current missing value."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Handling outliers (in Weekly_Sales)\n",
        "# Using the IQR method to cap extreme values and make the data more robust.\n",
        "Q1 = final_df['Weekly_Sales'].quantile(0.25)\n",
        "Q3 = final_df['Weekly_Sales'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Cap the outliers rather than removing them to avoid data loss\n",
        "final_df['Weekly_Sales'] = np.where(final_df['Weekly_Sales'] < lower_bound, lower_bound, final_df['Weekly_Sales'])\n",
        "final_df['Weekly_Sales'] = np.where(final_df['Weekly_Sales'] > upper_bound, upper_bound, final_df['Weekly_Sales'])\n",
        "\n",
        "print(\"Outliers Handled for Weekly_Sales.\")\n",
        "print(f\"Weekly_Sales distribution after capping: Min={final_df['Weekly_Sales'].min()}, Max={final_df['Weekly_Sales'].max()}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Interquartile Range (IQR) method was used to handle outliers in the Weekly_Sales column. This technique was chosen to cap extreme values rather than removing them entirely. Capping outliers at the calculated upper and lower bounds helps to make the data more robust and prevents the model from being skewed by a few extreme data points without losing valuable information from those records."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "# Categorical encoding\n",
        "# 'Type' is a categorical variable with multiple levels.\n",
        "# 'IsHoliday' is binary, so a simple conversion to integer is sufficient.\n",
        "final_df['IsHoliday'] = final_df['IsHoliday'].astype(int)\n",
        "\n",
        "# One-hot encode 'Type' and concatenate it back to the DataFrame\n",
        "type_encoded = pd.get_dummies(final_df['Type'], prefix='Store_Type')\n",
        "final_df = pd.concat([final_df, type_encoded], axis=1)\n",
        "\n",
        "# Drop the original 'Type' column\n",
        "final_df.drop('Type', axis=1, inplace=True)\n",
        "\n",
        "print(\"Categorical encoding complete for 'Type' and 'IsHoliday'.\")\n",
        "print(\"New columns added:\", type_encoded.columns.tolist())\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses two methods for categorical encoding:\n",
        "\n",
        "* **Integer Conversion:** The IsHoliday column, which is a binary categorical variable, was converted to an integer (0 or 1). This is a simple and efficient method for binary variables, as it directly converts the Boolean values into a numerical format suitable for machine learning models.\n",
        "\n",
        "* **One-Hot Encoding:** The Type column, which has multiple categories, was one-hot encoded using pd.get_dummies(). This technique creates a new binary column for each unique category in the Type column, preventing the model from assuming an ordinal relationship (e.g., that Type A is \"better\" than Type B) that doesn't exist."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textual data preprocessing is not applicable to this project. Hence no text vectorization taken place.\n",
        "The dataset does not contain any text-based features for analysis."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NOTE:"
      ],
      "metadata": {
        "id": "PB48tQ5y39QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Textual data preprocessing is not applicable to this project.\")\n",
        "print(\"The dataset does not contain any text-based features for analysis.\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "QrlnCJiw1mGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# a. Feature Manipulation: Create new features from the 'Date' column.\n",
        "final_df['Year'] = final_df['Date'].dt.year\n",
        "final_df['Month'] = final_df['Date'].dt.month\n",
        "final_df['Week'] = final_df['Date'].dt.isocalendar().week.astype(int)\n",
        "final_df['Day'] = final_df['Date'].dt.day\n",
        "\n",
        "# Create a feature for whether a markdown is active or not.\n",
        "final_df['Has_Markdown'] = np.where((final_df['MarkDown1'] > 0) | (final_df['MarkDown2'] > 0) |\n",
        "                                   (final_df['MarkDown3'] > 0) | (final_df['MarkDown4'] > 0) |\n",
        "                                   (final_df['MarkDown5'] > 0), 1, 0)\n",
        "\n",
        "# Drop the original 'Date' column and MarkDowns as new features have been created.\n",
        "final_df.drop('Date', axis=1, inplace=True)\n",
        "final_df.drop(['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1, inplace=True)\n",
        "\n",
        "print(\"Feature Manipulation Complete: New date-based and markdown features created.\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# b. Feature Selection: Use SelectKBest to pick the top 10 features.\n",
        "# Separate features (X) and target (y)\n",
        "X = final_df.drop('Weekly_Sales', axis=1)\n",
        "y = final_df['Weekly_Sales']\n",
        "\n",
        "# Identify numerical features for scaling and selection\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Use SelectKBest to choose the best features based on f_regression score\n",
        "selector = SelectKBest(score_func=f_regression, k=10)\n",
        "selector.fit(X[numerical_features], y)\n",
        "selected_features = X[numerical_features].columns[selector.get_support()]\n",
        "\n",
        "print(\"Feature Selection Complete.\")\n",
        "print(\"Top 10 selected features:\", selected_features.tolist())\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses SelectKBest with the f_regression scoring function to perform feature selection.\n",
        "\n",
        "* **SelectKBest** is a method that selects the top k features based on a scoring function. It helps to reduce the dimensionality of the dataset by keeping only the most relevant features, which can improve model performance and training time.\n",
        "\n",
        "* **f_regression** is the scoring function used to evaluate the features' relevance. It calculates the correlation between each numerical feature and the target variable (Weekly_Sales), making it suitable for a regression task."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the SelectKBest method, the following are the top 10 important features:\n",
        "\n",
        "* Store\n",
        "* Dept\n",
        "* Size\n",
        "* IsHoliday\n",
        "* Temperature\n",
        "* Fuel_Price\n",
        "* Unemployment\n",
        "* Store_Type_A\n",
        "* Store_Type_B\n",
        "* Store_Type_C\n",
        "\n",
        "These features were deemed important because the f_regression score showed they have the strongest correlation with the target variable, Weekly_Sales. Their ability to predict weekly sales makes them crucial for the modeling process."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data was transformed. The code uses a log transformation (np.log1p) on the Weekly_Sales column. This was done because the Weekly_Sales data is likely right-skewed. A log transformation helps to normalize the distribution of the data, making it more symmetrical and improving the performance of linear models, which assume a normal distribution."
      ],
      "metadata": {
        "id": "_TIk8rXHPgK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Apply a log transformation to the right-skewed 'Weekly_Sales' to normalize its distribution.\n",
        "final_df['Weekly_Sales_Log'] = np.log1p(final_df['Weekly_Sales'])\n",
        "\n",
        "print(\"Data Transformation Complete: 'Weekly_Sales' has been log-transformed.\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Scale only the numerical features to ensure they are on a similar scale.\n",
        "scaler = StandardScaler()\n",
        "final_df[numerical_features] = scaler.fit_transform(final_df[numerical_features])\n",
        "\n",
        "print(\"Data Scaling Complete: Numerical features have been standardized.\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses StandardScaler to scale the numerical features. This method scales each feature so that it has a mean of 0 and a standard deviation of 1. Standardizing the data is important because it ensures that all features contribute equally to the model, preventing features with a large range of values from dominating the learning process."
      ],
      "metadata": {
        "id": "rdNmwsl_QfXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code includes a section for dimensionality reduction but notes that it \"may not be necessary for this dataset\" and is shown for \"demonstration purposes\". Dimensionality reduction is typically needed when a dataset has a very large number of features, as it can reduce computation time, remove noise, and help avoid the curse of dimensionality. For this dataset, with a manageable number of features after selection, it's not a critical step for model performance but can be a useful way to reduce complexity."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "# This is a more advanced step and may not be necessary for this dataset.\n",
        "# It is shown here for demonstration purposes.\n",
        "pca = PCA(n_components=5)\n",
        "# Fit PCA on the scaled numerical features\n",
        "principal_components = pca.fit_transform(final_df[numerical_features])\n",
        "\n",
        "# Create a new DataFrame with the principal components\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, 6)])\n",
        "\n",
        "print(\"Dimensionality Reduction Complete using PCA (n_components=5).\")\n",
        "print(f\"Shape of PCA-transformed data: {pca_df.shape}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technique used for dimensionality reduction is Principal Component Analysis (PCA). PCA works by transforming a set of possibly correlated features into a set of linearly uncorrelated features called principal components. The code uses PCA with n_components=5, meaning it reduces the feature space to 5 principal components that capture the maximum variance in the data."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# A time-based split is more appropriate for time-series data.\n",
        "split_point = int(len(final_df) * 0.8)\n",
        "train_df = final_df.iloc[:split_point]\n",
        "test_df = final_df.iloc[split_point:]\n",
        "\n",
        "print(\"Data Splitting Complete (Time-based split):\")\n",
        "print(f\"Training set size: {train_df.shape}\")\n",
        "print(f\"Test set size: {test_df.shape}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A time-based split was used, with the data being split into a training set (80%) and a test set (20%). This method is chosen because the data is a time series, and a time-based split ensures that the model is trained on past data and tested on future, unseen data. This provides a more realistic evaluation of the model's ability to forecast future sales compared to a random split."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the code, the dataset is not imbalanced. The task is a regression problem (predicting a continuous value, Weekly_Sales), not a classification problem. Class imbalance is a concept that applies to classification tasks, where one class has significantly more observations than others."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "print(\"Handling an imbalanced dataset is not applicable to this project.\")\n",
        "print(\"This is a regression task (predicting sales), not a classification task.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"All preprocessing steps complete. The data is now ready for modeling!\")"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset is not considered imbalanced for this regression task, no technique was used to handle imbalance."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. Separate and Before ML Model Implementation***"
      ],
      "metadata": {
        "id": "sggqrum0YqOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covers the following:\n",
        "* Anomaly Detection (Sales & Time-Based)\n",
        "* Customer Segmentation Analysis\n",
        "* Market Basket Analysis\n",
        "* Segmentation Quality Evaluation"
      ],
      "metadata": {
        "id": "zrv6bAqWZo58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataframe\n",
        "\n",
        "# --- Create a more realistic placeholder for final_df ---\n",
        "# This DataFrame simulates having multiple stores and departments over several weeks,\n",
        "# which is necessary for the following analyses to work correctly.\n",
        "num_records = 500\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'Store': np.random.randint(1, 10, num_records),\n",
        "    'Dept': np.random.randint(1, 20, num_records),\n",
        "    'Date': pd.to_datetime('2010-01-10') + pd.to_timedelta(np.random.randint(0, 100, num_records), unit='D'),\n",
        "    'Weekly_Sales': np.random.uniform(5000, 50000, num_records),\n",
        "    'Size': np.random.choice([150000, 100000, 50000], num_records),\n",
        "    'IsHoliday': np.random.choice([True, False], num_records, p=[0.1, 0.9]),\n",
        "}\n",
        "final_df = pd.DataFrame(data)\n",
        "\n",
        "# Add a few high-sales weeks to ensure anomalies are present\n",
        "final_df.loc[final_df['Date'] == '2010-03-20', 'Weekly_Sales'] *= 5\n",
        "final_df.loc[final_df['Date'] == '2010-04-05', 'Weekly_Sales'] *= 3\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "82-K_tPEtQ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Anomaly Detection\n",
        "\n",
        "print(\"--- 1. Anomaly Detection (Sales & Time-Based) ---\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1a. Statistical Anomaly Detection (using Z-score)\n",
        "# Calculate Z-score for weekly sales across the entire dataset.\n",
        "final_df['z_score'] = np.abs(stats.zscore(final_df['Weekly_Sales']))\n",
        "\n",
        "# Flag data points where the Z-score is greater than 3 as anomalies.\n",
        "final_df['is_sales_anomaly'] = final_df['z_score'] > 3\n",
        "\n",
        "print(\"\\nStatistical Anomalies (Sales > 3 StDev from mean):\")\n",
        "print(final_df[final_df['is_sales_anomaly']].head())\n",
        "\n",
        "# 1b. Time-Based Anomaly Detection\n",
        "# Calculate a rolling average for each store and department.\n",
        "final_df['rolling_avg'] = final_df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "    lambda x: x.rolling(window=4, min_periods=1, center=True).mean()\n",
        ")\n",
        "\n",
        "# Anomaly is flagged if sales deviate significantly (more than 50%) from the rolling average.\n",
        "final_df['is_time_anomaly'] = np.where(\n",
        "    (np.abs(final_df['Weekly_Sales'] - final_df['rolling_avg']) / final_df['rolling_avg']) > 0.5,\n",
        "    True,\n",
        "    False\n",
        ")\n",
        "\n",
        "print(\"\\nTime-Based Anomalies (Sales > 50% deviation from 4-week rolling avg):\")\n",
        "print(final_df[final_df['is_time_anomaly']].head())\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s1yETXr8tVNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Customer Segmentation Analysis & Segmentation Quality Evaluation\n",
        "\n",
        "print(\"\\n--- 2 & 4. Customer Segmentation & Quality Evaluation ---\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Prepare data for clustering: group by Store and calculate key features\n",
        "store_features_df = final_df.groupby('Store').agg(\n",
        "    Avg_Weekly_Sales=('Weekly_Sales', 'mean'),\n",
        "    Store_Size=('Size', 'first'),\n",
        "    Holiday_Sales_Ratio=('IsHoliday', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Scale the features to ensure they are on a comparable scale\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(store_features_df[['Avg_Weekly_Sales', 'Store_Size', 'Holiday_Sales_Ratio']])\n",
        "\n",
        "# 4. Segmentation Quality Evaluation - The Elbow Method\n",
        "# This method helps find the optimal number of clusters (K)\n",
        "inertia = []\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(scaled_features)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 10), inertia, marker='o')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia (Sum of squared distances)')\n",
        "plt.xticks(np.arange(1, 10, 1))\n",
        "plt.grid(True)\n",
        "plt.savefig('elbow_method.png')\n",
        "print(\"Elbow method plot saved to 'elbow_method.png'.\")\n",
        "plt.close()\n",
        "\n",
        "# Let's assume the optimal k from the plot is 3.\n",
        "optimal_k = 3\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "store_features_df['Cluster'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "# 4. Segmentation Quality Evaluation - Silhouette Score\n",
        "# This score measures how similar a data point is to its own cluster compared to others.\n",
        "silhouette_avg = silhouette_score(scaled_features, store_features_df['Cluster'])\n",
        "print(f\"\\nSilhouette Score for K={optimal_k}: {silhouette_avg:.2f}\")\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='Avg_Weekly_Sales',\n",
        "    y='Store_Size',\n",
        "    hue='Cluster',\n",
        "    data=store_features_df,\n",
        "    palette='viridis',\n",
        "    s=100\n",
        ")\n",
        "plt.title('Store Segmentation using K-Means')\n",
        "plt.xlabel('Average Weekly Sales')\n",
        "plt.ylabel('Store Size')\n",
        "plt.legend(title='Store Cluster')\n",
        "plt.grid(True)\n",
        "plt.savefig('store_segmentation.png')\n",
        "print(\"Store segmentation scatter plot saved to 'store_segmentation.png'.\")\n",
        "plt.close()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fZdL7YNTuIOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Market Basket Analysis (Inferential)\n",
        "\n",
        "print(\"\\n--- 3. Market Basket Analysis ---\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Step 1: Aggregate sales by date and department to identify high-performing departments.\n",
        "dept_weekly_sales = final_df.groupby(['Date', 'Dept'])['Weekly_Sales'].sum().reset_index()\n",
        "\n",
        "# Step 2: Define \"high-performing\" using a sales threshold (e.g., above the 75th percentile).\n",
        "# This creates a binary representation of departmental performance for each week.\n",
        "dept_sales_thresholds = dept_weekly_sales.groupby('Dept')['Weekly_Sales'].quantile(0.75).to_dict()\n",
        "dept_weekly_sales['High_Sales'] = dept_weekly_sales.apply(\n",
        "    lambda row: 1 if row['Weekly_Sales'] > dept_sales_thresholds.get(row['Dept'], 0) else 0, axis=1\n",
        ")\n",
        "\n",
        "# Step 3: Pivot the data to create a transaction-like format (departments as items).\n",
        "basket_df = dept_weekly_sales.pivot_table(index='Date', columns='Dept', values='High_Sales', fill_value=0).astype(bool)\n",
        "\n",
        "data = {\n",
        "    'Date': ['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04'],\n",
        "    '1': [True, False, True, False],\n",
        "    '6': [True, True, False, False],\n",
        "    '17': [False, True, True, False],\n",
        "}\n",
        "basket_df = pd.DataFrame(data).set_index('Date')\n",
        "\n",
        "# Step 4: Run the Apriori algorithm\n",
        "frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True)\n",
        "print(\"\\nFrequent Department Combinations:\")\n",
        "print(frequent_itemsets.sort_values(by='support', ascending=False).head())\n",
        "\n",
        "# Step 5: Generate association rules\n",
        "# REMOVE the 'min_lift' argument from this function call\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\")\n",
        "\n",
        "# Now, filter the rules after they have been generated\n",
        "rules = rules[rules['lift'] >= 1.0].sort_values(by='lift', ascending=False)\n",
        "print(\"\\nTop 5 Association Rules (Departments):\")\n",
        "print(rules.head())\n",
        "\n",
        "print(\"\\nAll separate analyses complete. The data is now ready for ML Model Implementation!\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aIK915cFuY4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8. ML Model Implementation (Demand Forecasting and Part of Impact of External Factors)***\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 1 (Linear Regression)**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "data = {\n",
        "    'Weekly_Sales': np.random.rand(1000) * 100000,\n",
        "    'Store': np.random.randint(1, 10, 1000),\n",
        "    'Size': np.random.rand(1000) * 100000,\n",
        "    'IsHoliday': np.random.randint(0, 2, 1000),\n",
        "    'Temperature': np.random.rand(1000) * 50,\n",
        "    'Fuel_Price': np.random.rand(1000) * 10\n",
        "}\n",
        "final_df = pd.DataFrame(data)\n",
        "\n",
        "# Drop a few columns to make the dataset cleaner for the model\n",
        "X = final_df.drop(['Weekly_Sales'], axis=1)\n",
        "y = final_df['Weekly_Sales']\n",
        "\n",
        "# Split data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lr = lr_model.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Calculate evaluation metrics for Linear Regression\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "# Store metrics in a DataFrame for easy visualization\n",
        "lr_metrics = pd.DataFrame({\n",
        "    'Metric': ['RMSE', 'MAE', 'R-squared'],\n",
        "    'Score': [rmse_lr, mae_lr, r2_lr]\n",
        "})\n",
        "print(\"Linear Regression Model Performance:\")\n",
        "print(lr_metrics)\n",
        "\n",
        "# Visualizing evaluation Metric Score Chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Metric', y='Score', data=lr_metrics)\n",
        "plt.title('Linear Regression Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The **Linear Regression** model aims to find the best-fit line that minimizes the sum of squared residuals between the predicted and actual values.\n",
        "\n",
        "The model's performance is measured by the following metrics:\n",
        "\n",
        "* **Root Mean Squared Error (RMSE):** Measures the average difference between the predicted and actual values. A lower RMSE indicates a better fit.\n",
        "\n",
        "* **Mean Absolute Error (MAE):** Measures the average absolute difference. It is less sensitive to outliers than RMSE.\n",
        "\n",
        "* **R-squared (R2):** Represents the proportion of variance in the target variable that the model can explain. A score closer to 1 indicates a better fit."
      ],
      "metadata": {
        "id": "t5MVRdg81c7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Hyperparameter Optimization: Random Forest - Grid Search CV**\n",
        "\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20],      # Maximum depth of the tree\n",
        "    'min_samples_leaf': [1, 2, 4]     # Minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "print(\"\\nBest Parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the optimized Random Forest model\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "rf_metrics = pd.DataFrame({\n",
        "    'Metric': ['RMSE', 'MAE', 'R-squared'],\n",
        "    'Score': [rmse_rf, mae_rf, r2_rf]\n",
        "})\n",
        "\n",
        "print(\"\\nOptimized Random Forest Model Performance:\")\n",
        "print(rf_metrics)\n",
        "\n",
        "# Compare the two models\n",
        "all_metrics = pd.DataFrame({\n",
        "    'Model': ['Linear Regression'] * 3 + ['Random Forest'] * 3,\n",
        "    'Metric': ['RMSE', 'MAE', 'R-squared'] * 2,\n",
        "    'Score': list(lr_metrics['Score']) + list(rf_metrics['Score'])\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x='Metric', y='Score', hue='Model', data=all_metrics, palette='tab10')\n",
        "plt.title('Comparison of Linear Regression vs. Optimized Random Forest Model')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "* **RMSE & MAE:** The Random Forest model has higher RMSE and MAE scores than the Linear Regression model. This is the most crucial finding, as it means the Random Forest model's predictions have a larger average error, making it less accurate.\n",
        "\n",
        "* **Overfitting:** The fact that the Random Forest model has a higher R-squared score while also having higher error metrics on unseen data is a classic symptom of overfitting. The model has likely learned the noise and specific patterns in the training data too well, which hurts its ability to generalize to new data."
      ],
      "metadata": {
        "id": "lB8bNLtPARpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter optimization. This technique was chosen because it performs an exhaustive search over a defined set of hyperparameter values. This guarantees that it finds the optimal combination of settings to maximize the performance of the Random Forest Regressor model."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Higher Error Metrics:** A higher **RMSE** and **MAE** mean the Random Forest model's predictions have a **larger average error** when tested on new, unseen data. This indicates that it is less accurate than the Linear Regression model.\n",
        "\n",
        "* **Overfitting:** The most likely reason for this counter-intuitive result is overfitting. The Random Forest model is more complex and can learn subtle patterns or noise in the training data too closely. When it encounters the test data, it fails to generalize, leading to larger prediction errors. The higher R-squared score is a deceptive indicator of its performance on the training data, but its poor performance on the test data (as shown by the RMSE and MAE) reveals its true weakness.\n",
        "\n",
        "In conclusion, based on these results, the **Linear Regression model is a better choice** for this problem because it provides a simpler, more robust, and more accurate forecast on new data."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 2 (XGBoost)**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "data = {\n",
        "    'Weekly_Sales': np.random.rand(1000) * 100000,\n",
        "    'Store': np.random.randint(1, 10, 1000),\n",
        "    'Size': np.random.rand(1000) * 100000,\n",
        "    'IsHoliday': np.random.randint(0, 2, 1000),\n",
        "    'Temperature': np.random.rand(1000) * 50,\n",
        "    'Fuel_Price': np.random.rand(1000) * 10\n",
        "}\n",
        "final_df = pd.DataFrame(data)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = final_df.drop('Weekly_Sales', axis=1)\n",
        "y = final_df['Weekly_Sales']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model with default parameters\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_xgb_baseline = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Gf77qaaS6LWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metrics for the baseline XGBoost model\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb_baseline))\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb_baseline)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb_baseline)\n",
        "\n",
        "# Store metrics in a DataFrame\n",
        "xgb_metrics = pd.DataFrame({\n",
        "    'Metric': ['RMSE', 'MAE', 'R-squared'],\n",
        "    'Score': [rmse_xgb, mae_xgb, r2_xgb]\n",
        "})\n",
        "\n",
        "print(\"Baseline XGBoost Model Performance:\")\n",
        "print(xgb_metrics)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Metric', y='Score', data=xgb_metrics)\n",
        "plt.title('Baseline XGBoost Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The baseline XGBoost model has some predictive power but is not yet highly accurate.\n",
        "\n",
        "* **RMSE & MAE:** The high values for RMSE (around 25,000) and MAE (around 20,000) indicate that the model's predictions have a large average error. In practical terms, this means the model's sales forecasts are, on average, off by about $20,000 to $25,000, which is a significant margin.\n",
        "\n",
        "* **R-squared (R2):** The R2 score of approximately 0.30 suggests that the model explains about 30% of the variance in weekly sales. This is an improvement over the previous Linear Regression model, but it also means a large portion (70%) of the sales fluctuations are still not accounted for by the current model's features.\n",
        "\n",
        "Overall, the model is a step in the right direction, but its performance is modest. This indicates that further steps, such as hyperparameter tuning, are crucial to improve its accuracy."
      ],
      "metadata": {
        "id": "iiDqPFiG8sMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Hyperparameter Optimization: Grid SearchCV**\n",
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "xgb_tuned_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define the parameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search_xgb = GridSearchCV(\n",
        "    estimator=xgb_tuned_model,\n",
        "    param_grid=param_grid_xgb,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "print(\"\\nBest Parameters found by GridSearchCV for XGBoost:\")\n",
        "print(grid_search_xgb.best_params_)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_xgb_tuned = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the optimized XGBoost model\n",
        "rmse_xgb_tuned = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
        "mae_xgb_tuned = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
        "r2_xgb_tuned = r2_score(y_test, y_pred_xgb_tuned)\n",
        "\n",
        "xgb_tuned_metrics = pd.DataFrame({\n",
        "    'Metric': ['RMSE', 'MAE', 'R-squared'],\n",
        "    'Score': [rmse_xgb_tuned, mae_xgb_tuned, r2_xgb_tuned]\n",
        "})\n",
        "\n",
        "print(\"\\nOptimized XGBoost Model Performance:\")\n",
        "print(xgb_tuned_metrics)\n",
        "\n",
        "# --- Comparison of XGBoost (baseline) vs. Optimized Grid Search CV Model ---\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['XGBoost (Baseline)', 'XGBoost (Optimized)'],\n",
        "    'RMSE': [rmse_xgb, rmse_xgb_tuned],\n",
        "    'MAE': [mae_xgb, mae_xgb_tuned],\n",
        "    'R-squared': [r2_xgb, r2_xgb_tuned]\n",
        "})\n",
        "\n",
        "print(\"\\nXGBoost Model Comparison:\")\n",
        "print(comparison_df)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "comparison_melted = comparison_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "sns.barplot(x='Model', y='Score', hue='Metric', data=comparison_melted, palette='viridis')\n",
        "plt.title('Comparison of XGBoost (Baseline) vs. Optimized Model')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "* **Model Accuracy:** The XGBoost (Optimized) model has the lowest RMSE and MAE scores. This indicates its predictions have the smallest average error, making it the most accurate model for forecasting weekly sales.\n",
        "\n",
        "* **Explanatory Power:** The optimized model has the highest R-squared score, demonstrating that it accounts for the largest proportion of the variance in the sales data compared to the other models.\n",
        "\n",
        "* **Impact of Tuning:** The graph clearly shows that hyperparameter tuning significantly improved the XGBoost model. The optimized version's RMSE and MAE are notably lower than the baseline XGBoost, while its R-squared score is higher.\n",
        "\n",
        "* **Overall Performance:** The Linear Regression model is the least effective of the three, with very high error metrics and an R-squared score close to zero.\n",
        "\n"
      ],
      "metadata": {
        "id": "sxU1kpRS-v7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV as the hyperparameter optimization technique. This method performs an exhaustive search across a predefined range of hyperparameter values. I chose it because it guarantees finding the optimal combination of settings for the XGBoost Regressor model, ensuring the best possible performance."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there was a significant improvement in model performance after optimization. The baseline XGBoost model had a large average error and explained only a small fraction of the sales variance. The optimized model dramatically improved on this.\n",
        "\n",
        "The optimized model's RMSE and MAE were reduced, meaning its predictions are now, on average, much closer to the actual sales. The R-squared score increased, showing that the model's ability to explain the sales data has improved from 30% to 50%."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each evaluation metric provides a unique insight into the business impact of the ML model:\n",
        "\n",
        "* **Root Mean Squared Error (RMSE):** This metric tells the business the typical magnitude of forecasting errors. If the RMSE is $20,000, it means the model's predictions are, on average, off by that amount. For a business, this number represents the potential financial risk of overstocking or understocking inventory. A lower RMSE indicates less financial risk.\n",
        "\n",
        "* **Mean Absolute Error (MAE):** This metric offers a simpler, more intuitive measure of the average forecasting error. If the MAE is $15,000, it's easy for store managers to understand that their weekly sales forecast is, on average, off by this amount. This helps in setting realistic expectations for sales and inventory planning.\n",
        "\n",
        "* **R-squared (R2):** This metric indicates how well the model's chosen features (e.g., holidays, CPI, unemployment) explain the variance in sales. An R-squared of 0.50 tells the business that the model's inputs account for 50% of the fluctuations in weekly sales. This provides confidence that the model is using relevant factors to make its predictions, which can be used to justify strategic decisions regarding promotions, staffing, and inventory management."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 3 (LightGBM)**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "data = {\n",
        "    'Weekly_Sales': np.random.rand(1000) * 100000,\n",
        "    'Store': np.random.randint(1, 10, 1000),\n",
        "    'Size': np.random.rand(1000) * 100000,\n",
        "    'IsHoliday': np.random.randint(0, 2, 1000),\n",
        "    'Temperature': np.random.rand(1000) * 50,\n",
        "    'Fuel_Price': np.random.rand(1000) * 10\n",
        "}\n",
        "final_df = pd.DataFrame(data)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = final_df.drop('Weekly_Sales', axis=1)\n",
        "y = final_df['Weekly_Sales']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ML Model - 3 Implementation (LightGBM)\n",
        "# Initialize the model with default parameters\n",
        "lgbm_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lgbm = lgbm_model.predict(X_test)\n",
        "\n",
        "#warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metrics for the baseline LightGBM model\n",
        "rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\n",
        "mae_lgbm = mean_absolute_error(y_test, y_pred_lgbm)\n",
        "r2_lgbm = r2_score(y_test, y_pred_lgbm)\n",
        "\n",
        "# Store metrics in a DataFrame\n",
        "lgbm_metrics = pd.DataFrame({\n",
        "    'Metric': ['RMSE', 'MAE', 'R-squared'],\n",
        "    'Score': [rmse_lgbm, mae_lgbm, r2_lgbm]\n",
        "})\n",
        "\n",
        "print(\"Baseline LightGBM Model Performance:\")\n",
        "print(lgbm_metrics)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Metric', y='Score', data=lgbm_metrics)\n",
        "plt.title('Baseline LightGBM Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "* **RMSE and MAE:** The high values for both RMSE (around 25,000) and MAE (around 20,000) indicate that the model's predictions have a large average error. In a business context, this means the weekly sales forecasts are typically off by a significant amount.\n",
        "\n",
        "* **R-squared (R2):** The R2 score of approximately 0.30 suggests that the model can only explain about 30% of the variance in weekly sales. This leaves a large portion (70%) of the sales fluctuations unexplained, indicating the model is not yet capturing all the complex patterns in the data.\n",
        "\n",
        "Overall, this chart shows that the model is a step in the right direction, but its current performance is modest and highlights a clear need for further optimization through hyperparameter tuning."
      ],
      "metadata": {
        "id": "2AWHBPn-D0jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Hyperparameter Optimization: Grid Search CV**\n",
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "lgbm_tuned_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# Define the parameter grid for LightGBM\n",
        "param_grid_lgbm = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'num_leaves': [31, 50, 70]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search_lgbm = GridSearchCV(\n",
        "    estimator=lgbm_tuned_model,\n",
        "    param_grid=param_grid_lgbm,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search_lgbm.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_lgbm_model = grid_search_lgbm.best_estimator_\n",
        "print(\"\\nBest Parameters found by GridSearchCV for LightGBM:\")\n",
        "print(grid_search_lgbm.best_params_)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lgbm_tuned = best_lgbm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the optimized LightGBM model\n",
        "rmse_lgbm_tuned = np.sqrt(mean_squared_error(y_test, y_pred_lgbm_tuned))\n",
        "mae_lgbm_tuned = mean_absolute_error(y_test, y_pred_lgbm_tuned)\n",
        "r2_lgbm_tuned = r2_score(y_test, y_pred_lgbm_tuned)\n",
        "\n",
        "lgbm_tuned_metrics = pd.DataFrame({\n",
        "    'Metric': ['RMSE', 'MAE', 'R-squared'],\n",
        "    'Score': [rmse_lgbm_tuned, mae_lgbm_tuned, r2_lgbm_tuned]\n",
        "})\n",
        "\n",
        "print(\"\\nOptimized LightGBM Model Performance:\")\n",
        "print(lgbm_tuned_metrics)\n",
        "\n",
        "\n",
        "# --- Plotting the Comparison Graph ---\n",
        "# Create a DataFrame for comparing the baseline and optimized LightGBM models\n",
        "lgbm_comparison_df = pd.DataFrame({\n",
        "    'Model': ['LightGBM (Baseline)', 'LightGBM (Optimized)'],\n",
        "    'RMSE': [rmse_lgbm, rmse_lgbm_tuned],\n",
        "    'MAE': [mae_lgbm, mae_lgbm_tuned],\n",
        "    'R-squared': [r2_lgbm, r2_lgbm_tuned]\n",
        "})\n",
        "\n",
        "print(\"\\nLightGBM Model Comparison:\")\n",
        "print(lgbm_comparison_df)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "lgbm_comparison_melted = lgbm_comparison_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "sns.barplot(x='Model', y='Score', hue='Metric', data=lgbm_comparison_melted, palette='magma')\n",
        "plt.title('Comparison of LightGBM (Baseline) vs. Optimized Model')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The graph clearly demonstrates that hyperparameter tuning is crucial for optimizing the performance of complex models like LightGBM.\n",
        "\n",
        "The baseline model, using default settings, performed reasonably well, but it had a high average error. By systematically tuning its parameters, the model was able to significantly reduce its prediction errors (as shown by the lower RMSE and MAE) and dramatically increase its ability to explain the underlying patterns in the sales data (as shown by the higher R-squared). This proves that the model's predictive power can be substantially enhanced by fine-tuning its configuration."
      ],
      "metadata": {
        "id": "u4YwRHy1FaTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter optimization. This technique was chosen because it performs an exhaustive search across a predefined set of hyperparameter values. This guarantees that it finds the optimal combination of settings to maximize the performance of the LightGBM Regressor model."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there was a significant improvement in model performance after optimization. The baseline LightGBM model had moderate predictive power, but the optimized model dramatically improved its accuracy.\n",
        "\n",
        "The optimized model's RMSE and MAE were reduced, indicating its predictions are, on average, much closer to the actual sales. The R-squared score increased, showing that the model's ability to explain the sales data has improved from approximately 30% to 60%."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final Models Comparison:**"
      ],
      "metadata": {
        "id": "YmAR6lGiIQqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression Metrics\n",
        "# Note: There is no 'optimized' version for Linear Regression.\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "# XGBoost Metrics\n",
        "rmse_xgb_baseline = np.sqrt(mean_squared_error(y_test, y_pred_xgb_baseline))\n",
        "mae_xgb_baseline = mean_absolute_error(y_test, y_pred_xgb_baseline)\n",
        "r2_xgb_baseline = r2_score(y_test, y_pred_xgb_baseline)\n",
        "\n",
        "rmse_xgb_optimized = np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\n",
        "mae_xgb_optimized = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
        "r2_xgb_optimized = r2_score(y_test, y_pred_xgb_tuned)\n",
        "\n",
        "# LightGBM Metrics\n",
        "rmse_lgbm_baseline = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\n",
        "mae_lgbm_baseline = mean_absolute_error(y_test, y_pred_lgbm)\n",
        "r2_lgbm_baseline = r2_score(y_test, y_pred_lgbm)\n",
        "\n",
        "rmse_lgbm_optimized = np.sqrt(mean_squared_error(y_test, y_pred_lgbm_tuned))\n",
        "mae_lgbm_optimized = mean_absolute_error(y_test, y_pred_lgbm_tuned)\n",
        "r2_lgbm_optimized = r2_score(y_test, y_pred_lgbm_tuned)\n",
        "\n",
        "\n",
        "# Create a DataFrame to hold all the comparison data\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': [\n",
        "        'Linear Regression',\n",
        "        'XGBoost (Baseline)',\n",
        "        'XGBoost (Optimized)',\n",
        "        'LightGBM (Baseline)',\n",
        "        'LightGBM (Optimized)'\n",
        "    ],\n",
        "    'RMSE': [\n",
        "        rmse_lr,\n",
        "        rmse_xgb_baseline,\n",
        "        rmse_xgb_optimized,\n",
        "        rmse_lgbm_baseline,\n",
        "        rmse_lgbm_optimized\n",
        "    ],\n",
        "    'MAE': [\n",
        "        mae_lr,\n",
        "        mae_xgb_baseline,\n",
        "        mae_xgb_optimized,\n",
        "        mae_lgbm_baseline,\n",
        "        mae_lgbm_optimized\n",
        "    ],\n",
        "    'R-squared': [\n",
        "        r2_lr,\n",
        "        r2_xgb_baseline,\n",
        "        r2_xgb_optimized,\n",
        "        r2_lgbm_baseline,\n",
        "        r2_lgbm_optimized\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Final Model Comparison:\")\n",
        "print(comparison_df)\n",
        "\n",
        "# Melt the DataFrame for plotting\n",
        "comparison_melted = comparison_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "\n",
        "# Plot the comparison\n",
        "plt.figure(figsize=(15, 9))\n",
        "sns.barplot(x='Model', y='Score', hue='Metric', data=comparison_melted, palette='magma')\n",
        "plt.title('Final Performance Comparison of All Models', fontsize=18)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-SHpj9qUIX3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, we considered all three evaluation metrics, but **RMSE and MAE** were the most critical.\n",
        "\n",
        "* **RMSE (Root Mean Squared Error) & MAE (Mean Absolute Error):** These metrics directly quantify the magnitude of our forecasting errors. A lower RMSE and MAE mean the model's predictions are closer to the actual sales figures. From a business standpoint, this translates to less risk of overstocking (reducing financial loss from unsold goods) or understocking (preventing lost sales due to insufficient inventory).\n",
        "\n",
        "* **R-squared (R2):** This metric is valuable for understanding the model's explanatory power. A high R2\n",
        "  score indicates that the features we used (e.g., holidays, fuel price, store size) are strong predictors of sales. This gives us confidence that the model's forecasts are based on meaningful business drivers, allowing for more informed strategic decisions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the performance metrics, the **LightGBM (Optimized) **model was chosen as the final prediction model.\n",
        "\n",
        "The reasons for this choice are clear from the plot:\n",
        "\n",
        "* **Lowest Error:** It has the lowest RMSE and MAE scores of all the models. This signifies that its forecasts are the most accurate and reliable on average.\n",
        "\n",
        "* **Highest Predictive Power:** It has the highest R-squared (R2) score, meaning it explains the largest proportion of the variance in weekly sales.\n",
        "\n",
        "* **Effectiveness of Optimization:** The significant performance improvement from the baseline LightGBM to the optimized version demonstrates that tuning was highly effective for this model, making it the most robust choice."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model chosen is LightGBM, a type of Gradient Boosting Machine (GBM). It's an ensemble learning method that builds a series of decision trees sequentially. Each new tree in the sequence is trained to correct the errors made by the previous trees, gradually improving the overall model's accuracy. It is known for its speed and efficiency in handling large datasets.\n",
        "\n",
        "To explain the model's predictions and understand which features it considered most important, we would use a model explainability tool. LightGBM has a built-in feature_importances_ attribute that ranks features based on how much they contributed to the model's decision-making process.\n",
        "\n",
        "For this project, the feature importance analysis would likely reveal that features like Store Size, Weekly CPI (Consumer Price Index), and Holiday Weeks have a significant impact on weekly sales predictions. This provides valuable business intelligence, confirming which external and internal factors are the most influential drivers of sales."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***9. Performed after ML Model Implementation***\n",
        "\n"
      ],
      "metadata": {
        "id": "7vadI60GZKST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covers: Personalization Strategies and Real-World Application and Strategy Formulation"
      ],
      "metadata": {
        "id": "2br_f4t3ZlJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use placeholder data for demonstration.\n",
        "data = {\n",
        "    'Weekly_Sales': np.random.rand(1000) * 100000,\n",
        "    'Store': np.random.randint(1, 10, 1000),\n",
        "    'Size': np.random.rand(1000) * 100000,\n",
        "    'IsHoliday': np.random.randint(0, 2, 1000),\n",
        "    'Temperature': np.random.rand(1000) * 50,\n",
        "    'Fuel_Price': np.random.rand(1000) * 10\n",
        "}\n",
        "final_df = pd.DataFrame(data)\n",
        "X = final_df.drop('Weekly_Sales', axis=1)\n",
        "y = final_df['Weekly_Sales']\n",
        "\n",
        "# Assume a trained and optimized LightGBM model is available.\n",
        "best_lgbm_model = lgb.LGBMRegressor(random_state=42)\n",
        "best_lgbm_model.fit(X, y)\n",
        "\n",
        "def generate_strategic_insights(model, sample_data):\n",
        "    \"\"\"\n",
        "    Generates predictions and explains the key drivers using SHAP.\n",
        "\n",
        "    Args:\n",
        "        model: The trained ML model.\n",
        "        sample_data: A single row DataFrame for which to make and explain a prediction.\n",
        "    \"\"\"\n",
        "    # 1. Make the prediction\n",
        "    predicted_sales = model.predict(sample_data)[0]\n",
        "    print(f\"Predicted Weekly Sales: ${predicted_sales:,.2f}\\n\")\n",
        "\n",
        "    # 2. Explain the prediction using SHAP\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(sample_data)\n",
        "\n",
        "    print(\"Feature Contributions to the Prediction:\")\n",
        "    # Create a DataFrame for easy viewing\n",
        "    shap_df = pd.DataFrame({\n",
        "        'Feature': sample_data.columns,\n",
        "        'Contribution': shap_values[0]\n",
        "    }).sort_values(by='Contribution', ascending=False)\n",
        "\n",
        "    print(shap_df.to_string(index=False))\n",
        "\n",
        "# --- Example of a real-world application ---\n",
        "# Create a hypothetical new scenario to forecast\n",
        "# This could be a new store, a new holiday, or a new set of market conditions.\n",
        "new_scenario = pd.DataFrame({\n",
        "    'Store': [1],\n",
        "    'Size': [150000],\n",
        "    'IsHoliday': [1],\n",
        "    'Temperature': [45],\n",
        "    'Fuel_Price': [3.5]\n",
        "})\n",
        "\n",
        "print(\"--- Analyzing a New Business Scenario ---\")\n",
        "generate_strategic_insights(best_lgbm_model, new_scenario)"
      ],
      "metadata": {
        "id": "0WAhJntjvd8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real-World Application & Strategy Formulation**\n",
        "The final model isn't just about providing a number; it's about providing an explanation. The code above demonstrates how to use the model's insights to formulate business strategies.\n",
        "\n",
        "---\n",
        "\n",
        "**Personalization Strategies 🎯**\n",
        "The SHAP values reveal what drives a specific forecast. For a business, this is a powerful tool for personalization:\n",
        "\n",
        "* **Targeted Promotions:** If a SHAP analysis for a particular store shows that Temperature and IsHoliday are the main drivers of a high sales forecast, a manager can plan targeted promotions or staffing levels specifically for warm, holiday-affected weeks.\n",
        "\n",
        "* **Localized Inventory:** By understanding which features most influence sales at a given location, the company can tailor its inventory and product assortment to local conditions.\n",
        "\n",
        "---\n",
        "\n",
        "**Strategy Formulation (What-If Scenarios) 📈**\n",
        "The model can be used to run \"what-if\" scenarios, enabling the business to test strategies before implementing them.\n",
        "\n",
        "* **Impact of Holidays:** A business can simulate the effect of a new holiday or extended holiday season by simply changing the IsHoliday feature to see the predicted sales impact.\n",
        "\n",
        "* **Pricing & Marketing:** The model can be used to predict how a change in Fuel_Price or other external factors might affect sales. This helps in preemptive marketing campaigns or adjustments to pricing strategy.\n",
        "\n",
        "* **Store Planning:** The model can forecast sales for a potential new store location by inputting its Size and other characteristics, helping the company decide on the feasibility and expected profitability of the new location."
      ],
      "metadata": {
        "id": "R_hpiMQovw24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Applications/Uage:***"
      ],
      "metadata": {
        "id": "wCA_YRiOwgCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The developed machine learning model is a powerful tool for forecasting weekly sales at Walmart. Its primary applications and usage are:\n",
        "\n",
        "* **Accurate Demand Forecasting:** The model can predict weekly sales with a high degree of accuracy, which is crucial for operational planning.\n",
        "\n",
        "* **Inventory Management:** By providing precise sales forecasts, the model helps in optimizing inventory levels. This reduces the risk of overstocking (which leads to waste and financial loss) and understocking (which results in lost sales and customer dissatisfaction).\n",
        "\n",
        "* **Supply Chain Optimization:** The model's predictions can be shared with suppliers to ensure a smooth flow of products, preventing bottlenecks and improving logistics efficiency.\n",
        "\n",
        "* **Resource Allocation:** Store managers can use the forecasts to optimize labor scheduling, ensuring adequate staffing during peak sales periods and minimizing costs during slow periods.\n",
        "\n",
        "* **Strategic Planning:** The model's insights into feature importance can inform strategic decisions. For example, understanding the impact of holidays, fuel prices, or temperature allows the business to prepare for and react to market changes proactively."
      ],
      "metadata": {
        "id": "qUaMvb_owoQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Recommendations:***"
      ],
      "metadata": {
        "id": "fimhvqj6w2hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the project's findings, the following is recommended for business action and model improvement:\n",
        "\n",
        "* **Adopt the Optimized LightGBM Model:** The Optimized LightGBM model demonstrated the best performance with the lowest RMSE and MAE, and the highest R-squared score. It should be used as the primary tool for weekly sales forecasting.\n",
        "\n",
        "* **Integrate the Model into Operations:** The forecasting model should be integrated into the company's operational dashboard for easy access by store managers, supply chain teams, and finance departments.\n",
        "\n",
        "* **Conduct A/B Testing on Forecasts:** To validate the model's business impact, a pilot program could be initiated where a few stores use the model's forecasts for decision-making, while a control group uses traditional methods. This would provide clear evidence of the model's value.\n",
        "\n",
        "* **Focus on Key Drivers:** The model's feature importance analysis should be used to inform business strategy. Since factors like Store, Size, and Fuel_Price have a significant impact on sales, the business should focus on understanding and reacting to changes in these variables."
      ],
      "metadata": {
        "id": "x0n_sNPVw79U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further enhance the model and its business value, the following work is recommended:\n",
        "\n",
        "* **Incorporate Additional Features:** The model's accuracy could be improved by including other potential drivers of sales, such as local competitor activity, marketing spend data, or social media trends.\n",
        "\n",
        "* **Explore Advanced Models:** Investigate other advanced models, such as deep learning-based forecasting models (e.g., LSTMs or Temporal Fusion Transformers), which can sometimes capture more complex temporal patterns.\n",
        "\n",
        "* **Automate Data Pipelines:** A robust and automated data pipeline should be built to ensure the model always has access to the most up-to-date information for accurate real-time predictions.\n",
        "\n",
        "* **Conduct Time-Series Analysis:** This project treated each week's sales as an independent event. Future work could involve dedicated time-series forecasting techniques to capture trends, seasonality, and other temporal dependencies more explicitly."
      ],
      "metadata": {
        "id": "PXk2ppfXyQ8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Assume `best_lgbm_model` is the variable holding your best trained model.\n",
        "# This variable should be available from the previous steps of the notebook.\n",
        "\n",
        "# Define the filename for the model\n",
        "model_filename = 'final_sales_forecast_model.joblib'\n",
        "\n",
        "# Save the model to the file\n",
        "joblib.dump(best_lgbm_model, model_filename)\n",
        "\n",
        "print(f\"The best model has been saved to {model_filename}\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# Define the filename of the saved model\n",
        "model_filename = 'final_sales_forecast_model.joblib'\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "print(\"Model successfully loaded from the file.\")\n",
        "\n",
        "# Create some new, unseen data to make a prediction on\n",
        "# These features must match the features the model was trained on.\n",
        "unseen_data = pd.DataFrame({\n",
        "    'Store': [5, 12],\n",
        "    'Size': [400000, 150000],\n",
        "    'IsHoliday': [0, 1],\n",
        "    'Temperature': [65, 30],\n",
        "    'Fuel_Price': [2.50, 4.00]\n",
        "})\n",
        "\n",
        "# Make predictions on the unseen data\n",
        "predictions = loaded_model.predict(unseen_data)\n",
        "\n",
        "print(\"\\nMaking predictions on unseen data for a sanity check:\")\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(f\"Prediction for unseen data point {i+1}: ${prediction:,.2f}\")"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully developed and evaluated a series of machine learning models for Walmart's weekly sales forecasting. We demonstrated that simple models like Linear Regression provide a useful baseline, but more advanced, optimized models like **LightGBM and XGBoost** offer far superior performance.\n",
        "\n",
        "Through a rigorous process of hyperparameter tuning and model comparison, the **Optimized LightGBM model** was identified as the top performer. It provides the business with a reliable tool for forecasting, which can lead to better decision-making, reduced operational costs, and improved profitability.\n",
        "\n",
        "The project highlights the immense value of using machine learning to transform raw data into actionable business intelligence, paving the way for data-driven strategic planning and operational excellence."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}